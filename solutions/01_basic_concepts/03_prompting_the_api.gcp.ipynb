{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c05fafa1fe34c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3 - Introduction to Gemini API and prompting\n",
    "Google makes their models available via the [Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal#python) and multiple SDKs.\n",
    "In this exercise, we will explore how to use the Python SDK to interact with these models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f0c7",
   "metadata": {},
   "source": [
    "If you haven't done so already, it's time to authenticate Google cloud.\n",
    "<details>\n",
    "<summary>Steps for authenticating Google Cloud</summary>\n",
    "\n",
    "1. Go to the [Google Cloud Console](https://console.cloud.google.com/).\n",
    "2. In the search bar at the top, search for the project ID used for this training. Your trainer will confirm what this is, but it probably starts with `academy-llm-applications-*`!\n",
    "3. In your Codespace terminal, run `gcloud auth login` and log in to your Google account.\n",
    "4. In the same terminal, run `gcloud auth application-default login` and log in to your Google account.\n",
    "5. Set the project ID by running `gcloud config set project <PROJECT_ID>` in the terminal, replacing `<PROJECT_ID>` with the project ID from step 2.\n",
    "6. Set the quota project by running `gcloud auth application-default set-quota-project <PROJECT_ID>` in the terminal, again replacing `<PROJECT_ID>` with the project ID from step 2.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596671c",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by initiating the chat model. We will use the `vertexai` package for this to connect to Google Cloud and the Vertexi AI API's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52e9eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, Content\n",
    "from IPython.display import Image, display\n",
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "vertexai.init(\n",
    "    project=os.environ[\"GCP_PROJECT_ID\"],\n",
    "    # location=os.environ[\"GCP_LOCATION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e205efe",
   "metadata": {},
   "source": [
    "We are now able to instantiate a Generative model. Let's instantiate Gemini 1.5 flash ‚ö°Ô∏è."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec14baf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM region: us-central1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "print(f\"LLM region: {model._location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eee0e5",
   "metadata": {},
   "source": [
    "This model we can use to generate some content. Let's get some ideas for your tonight's dinner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c4ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = model.generate_content(\n",
    "    \"Give me some ideas for dinner tonight.\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94391d",
   "metadata": {},
   "source": [
    "That's great! We now verified we could successfully connect to Gemini's API and generate text üéâ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fce235371fcc0cee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 3a: Vertex AI Gemini API - capabilities walkthrough\n",
    "Let's get familiar with features of the Gemini API. We will cover the following by showing you interactive code examples and explanations of each feature:\n",
    "\n",
    "1. Multimodal generation\n",
    "2. Using system prompts\n",
    "3. Using content generation parameters (temperature, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d22863",
   "metadata": {},
   "source": [
    "### 1. Multimodal generation\n",
    "Gemini models are multimodal, meaning they can process different types of data, such as text, images, audio, and video. Let's explore how to use this capability with an image. We will be using a test image from Google Cloud Storage bucket, but you can replace it with a local path to an image as well. \n",
    "\n",
    "First, let's select an image to use and take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51bd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_URL = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/scones.jpg\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(url=IMAGE_URL, width=500))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c96d4c",
   "metadata": {},
   "source": [
    "We'll need to prepare the image for use with model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9033d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Part.from_uri(IMAGE_URL, mime_type=\"image/jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225011ef",
   "metadata": {},
   "source": [
    "Now we can use our previously initialized model, together with a prompt, to get a description of the image we just prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ff568",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is this and can you tell me some interesting fact about it?\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    [prompt, image]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73a2d67",
   "metadata": {},
   "source": [
    "As you can see, the model successfully understood the context of the image and gave us useful information. This showcases how to use the Gemini API for multimodal inputs. \n",
    "\n",
    "Now let's move on to system prompts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d79a5",
   "metadata": {},
   "source": [
    "### 2. Using system prompts\n",
    "System prompts allow you to set the context and behaviour of the model. They are useful for creating specific personas or setting specific guidelines for the model to follow. Add below a system instruction when instantiating the model.\n",
    "\n",
    "Take a look at the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions) for some tips on writing system instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0465b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful and concise assistant that summarizes the input in three bullet points.\"\n",
    "prompt = \"The French Revolution was a period of social and political upheaval in late 1700's France. It eventually toppled the monarchy and led to the rise of Napoleon Bonaparte.\"\n",
    "\n",
    "# YOUR CODE HERE START\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\", system_instruction = [system_prompt])\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,    \n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272e17d",
   "metadata": {},
   "source": [
    "As you can see, the model followed the system prompt and returned the summary in three bullet points. System prompts are a powerful way to control the model's behavior and to tailor it to your specific needs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4607e",
   "metadata": {},
   "source": [
    "### 3. Using content generation parameters\n",
    "The Gemini API allows you to control the generation of text using several parameters such as `temperature`, `top_p` and `top_k`. These parameters affect the randomness and diversity of the generated content. Let's see what each of the do:\n",
    "\n",
    "*   `temperature`: Controls the randomness of the output. Higher values result in more random outputs and lower values will result in more deterministic outputs. The suggested range is 0.0 to 1.0 but higher values can also be used (up to 2.0).\n",
    "*   `top_p`: Controls the cumulative probability of the tokens selected. Lower values tend to lead to more focused generation, while higher values can result in more diverse outputs.\n",
    "*   `top_k`: Controls the number of tokens that can be selected. Higher values lead to more tokens being considered.\n",
    "\n",
    "Let's explore how temperature affects the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3225c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me something about the sky.\"\n",
    "print(\"Temperature 0.2\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config = {\n",
    "        # YOUR CODE HERE START\n",
    "        \"temperature\": 0.2\n",
    "        # YOUR CODE HERE END\n",
    "    }\n",
    ")\n",
    "print(response.text)\n",
    "\n",
    "print(\"\\nTemperature 1.0\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    # YOUR CODE HERE START\n",
    "    generation_config = {\"temperature\": 1.0}\n",
    "    # YOUR CODE HERE END\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2f440e",
   "metadata": {},
   "source": [
    "Try also re-running the cell several times to see how random the output of the model is. As you can see, the output of the model changes based on the temperature. A lower temperature makes the output more deterministic and focused, while a higher temperature makes the output more creative and diverse.\n",
    "\n",
    "You can experiment with the `top_p` and `top_k` parameters as well. \n",
    "\n",
    "This concludes this introduction into the Google Gemini Vertex AI API. You've learned how to connect to the API, generate text and process multimodal content, control the behaviour of the models with system prompts and control the output generation using temperature.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61c33f",
   "metadata": {},
   "source": [
    "## Optional: Exercise 3b - base model vs instruction fine-tuned model\n",
    "Let's make a CLI-based version of Gemini.\n",
    "\n",
    "Below, you will find some boilerplate code that takes care of asking the user for input and printing the conversation.\n",
    "However, there are still a few things missing like:\n",
    "- The content of the inital system prompt.\n",
    "- The API call to the model, which leads to a response from the chatbot based on the conversation so far.\n",
    "\n",
    "\n",
    "Can you finish it?\n",
    "\n",
    "<mark>Note: The text box should appear at the top of your IDE (VS Code) window.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51666c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# The initial system prompt.\n",
    "messages = [\n",
    "    Content(role='user', parts=[\n",
    "        Part.from_text('Hi there'),\n",
    "    ]),\n",
    "\n",
    "    # pass context to the model, simulating a system prompt, by mimicking a model response.\n",
    "    Content(role='model', parts=[\n",
    "        Part.from_text(\n",
    "            'Hi! I am an AI assistant that helps people with their every day tasks. How can I help you today?')\n",
    "    ])\n",
    "]\n",
    "\n",
    "chat = model.start_chat(history=messages)\n",
    "\n",
    "while True:\n",
    "    # Get the user's input.\n",
    "    user_message = input('User:').strip()\n",
    "\n",
    "    # Check whether to continue.\n",
    "    if len(user_message) == 0 or user_message == \"exit\":\n",
    "        print(\"exiting...\")\n",
    "        break\n",
    "    print(1)\n",
    "\n",
    "    # Send the user's message to the model.\n",
    "    # YOUR CODE HERE START\n",
    "    response = chat.send_message(user_message,\n",
    "                                 generation_config={\"temperature\": 1.0})\n",
    "    # YOUR CODE HERE END\n",
    "\n",
    "    assistant_message = response.text\n",
    "    print(2)\n",
    "\n",
    "    # Print the user input and response.\n",
    "    print(\"User:\", user_message)\n",
    "    print(\"AI:\", assistant_message)\n",
    "\n",
    "    # We need to wait a little bit of time for the text to render.\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba08ff8b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
