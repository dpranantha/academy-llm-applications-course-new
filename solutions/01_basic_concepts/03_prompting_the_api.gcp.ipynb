{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c05fafa1fe34c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3 - Introduction to Gemini API and prompting\n",
    "Google makes their models available via the [Gemini API](https://cloud.google.com/vertex-ai/generative-ai/docs/start/quickstarts/quickstart-multimodal#python) and multiple SDKs.\n",
    "In this exercise, we will explore how to use the Python SDK to interact with these models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984f0c7",
   "metadata": {},
   "source": [
    "If you haven't done so already, it's time to authenticate Google cloud.\n",
    "<details>\n",
    "<summary>Steps for authenticating Google Cloud</summary>\n",
    "\n",
    "1. Go to the [Google Cloud Console](https://console.cloud.google.com/).\n",
    "2. In the search bar at the top, search for the project ID used for this training. Your trainer will confirm what this is, but it probably starts with `academy-llm-applications-*`!\n",
    "3. In your Codespace terminal, run `gcloud auth login` and log in to your Google account.\n",
    "4. In the same terminal, run `gcloud auth application-default login` and log in to your Google account.\n",
    "5. Set the project ID by running `gcloud config set project <PROJECT_ID>` in the terminal, replacing `<PROJECT_ID>` with the project ID from step 2.\n",
    "6. Set the quota project by running `gcloud auth application-default set-quota-project <PROJECT_ID>` in the terminal, again replacing `<PROJECT_ID>` with the project ID from step 2.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7596671c",
   "metadata": {},
   "source": [
    "\n",
    "Let's start by initiating the chat model. We will use the `vertexai` package for this to connect to Google Cloud and the Vertexi AI API's."
   ]
  },
  {
   "cell_type": "code",
   "id": "52e9eef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:10:38.543922Z",
     "start_time": "2025-03-05T12:10:38.538080Z"
    }
   },
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, Part, Content\n",
    "from IPython.display import Image, display\n",
    "# load environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "%reload_ext dotenv\n",
    "%dotenv /Users/dpranantha/Projects/bol/training/academy-llm-applications-course/.env\n",
    "\n",
    "vertexai.init(\n",
    "    project=os.environ[\"GCP_PROJECT_ID\"],\n",
    "    location=os.environ[\"GCP_LOCATION\"]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:10:12.409127Z",
     "start_time": "2025-03-05T12:10:12.406661Z"
    }
   },
   "cell_type": "code",
   "source": "os.environ[\"GCP_PROJECT_ID\"]",
   "id": "3840157b268d1c53",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'academy-llm-applications-gcp'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "0e205efe",
   "metadata": {},
   "source": [
    "We are now able to instantiate a Generative model. Let's instantiate Gemini 1.5 flash ‚ö°Ô∏è."
   ]
  },
  {
   "cell_type": "code",
   "id": "ec14baf1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:10:43.048213Z",
     "start_time": "2025-03-05T12:10:43.046399Z"
    }
   },
   "source": [
    "\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\")\n",
    "\n",
    "print(f\"LLM region: {model._location}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM region: europe-west1\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "a9eee0e5",
   "metadata": {},
   "source": [
    "This model we can use to generate some content. Let's get some ideas for your tonight's dinner."
   ]
  },
  {
   "cell_type": "code",
   "id": "c7c4ea34",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:11:01.756426Z",
     "start_time": "2025-03-05T12:10:56.839325Z"
    }
   },
   "source": [
    "\n",
    "response = model.generate_content(\n",
    "    \"Give me some ideas for dinner tonight.\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Quick & Easy (30 minutes or less):**\n",
      "\n",
      "* **Pasta Aglio e Olio:** Garlic, olive oil, pasta, Parmesan cheese ‚Äì simple and delicious.\n",
      "* **Sheet Pan Chicken and Veggies:** Toss chicken and your favorite veggies (broccoli, carrots, potatoes) with olive oil, herbs, and spices, then roast.\n",
      "* **Quesadillas:** Endless variations ‚Äì cheese, beans, leftover chicken or steak, veggies.\n",
      "* **Tuna Melts:** A classic comfort food.\n",
      "* **Omelets or Frittatas:**  A great way to use up leftover veggies.\n",
      "\n",
      "\n",
      "**More Involved (45+ minutes):**\n",
      "\n",
      "* **Chicken Stir-fry:**  Choose your protein (chicken, tofu, shrimp) and your favorite veggies. Serve over rice or noodles.\n",
      "* **Tacos or Burritos:**  Customize with your favorite fillings and toppings.\n",
      "* **Shepherd's Pie:**  A hearty and comforting meal.\n",
      "* **Lasagna:** A classic crowd-pleaser (can be made ahead of time).\n",
      "* **Roast Chicken:** Simple and flavorful. Serve with roasted potatoes or a salad.\n",
      "\n",
      "\n",
      "**Ethnic Inspiration:**\n",
      "\n",
      "* **Indian Butter Chicken:** Creamy and flavorful.\n",
      "* **Thai Green Curry:**  Spicy and aromatic.\n",
      "* **Japanese Ramen:**  A comforting noodle soup.\n",
      "* **Mexican Chili:** Hearty and warming.\n",
      "* **Italian Pizza:** Homemade or takeout!\n",
      "\n",
      "\n",
      "To help me narrow it down, tell me:\n",
      "\n",
      "* **What ingredients do you have on hand?**\n",
      "* **How much time do you want to spend cooking?**\n",
      "* **What kind of cuisine are you in the mood for?**\n",
      "* **Any dietary restrictions or preferences?** (vegetarian, vegan, gluten-free, etc.)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "3f94391d",
   "metadata": {},
   "source": [
    "That's great! We now verified we could successfully connect to Gemini's API and generate text üéâ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fce235371fcc0cee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 3a: Vertex AI Gemini API - capabilities walkthrough\n",
    "Let's get familiar with features of the Gemini API. We will cover the following by showing you interactive code examples and explanations of each feature:\n",
    "\n",
    "1. Multimodal generation\n",
    "2. Using system prompts\n",
    "3. Using content generation parameters (temperature, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d22863",
   "metadata": {},
   "source": [
    "### 1. Multimodal generation\n",
    "Gemini models are multimodal, meaning they can process different types of data, such as text, images, audio, and video. Let's explore how to use this capability with an image. We will be using a test image from Google Cloud Storage bucket, but you can replace it with a local path to an image as well. \n",
    "\n",
    "First, let's select an image to use and take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "id": "ff51bd12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:12:04.710589Z",
     "start_time": "2025-03-05T12:12:04.707617Z"
    }
   },
   "source": [
    "IMAGE_URL = \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/scones.jpg\"\n",
    "\n",
    "# Display the image\n",
    "display(Image(url=IMAGE_URL, width=500))\n"
   ],
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/scones.jpg\" width=\"500\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "82c96d4c",
   "metadata": {},
   "source": [
    "We'll need to prepare the image for use with model."
   ]
  },
  {
   "cell_type": "code",
   "id": "df9033d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:12:13.473801Z",
     "start_time": "2025-03-05T12:12:13.471841Z"
    }
   },
   "source": [
    "image = Part.from_uri(IMAGE_URL, mime_type=\"image/jpeg\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "225011ef",
   "metadata": {},
   "source": [
    "Now we can use our previously initialized model, together with a prompt, to get a description of the image we just prepared."
   ]
  },
  {
   "cell_type": "code",
   "id": "5c4ff568",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:12:20.393037Z",
     "start_time": "2025-03-05T12:12:18.391235Z"
    }
   },
   "source": [
    "prompt = \"What is this and can you tell me some interesting fact about it?\"\n",
    "\n",
    "response = model.generate_content(\n",
    "    [prompt, image]\n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's a delicious-looking image of blueberry scones served with coffee and fresh blueberries.  The scones appear to be homemade and are scattered on parchment paper, along with fresh blueberries and peonies.  There's also a spoon with \"Let's Jam\" engraved on it.\n",
      "\n",
      "Here's an interesting fact about scones:\n",
      "\n",
      "**The origin of the name \"scone\" is debated.**  Some believe it comes from the Scottish word \"scon,\" meaning \"to cut,\" referring to the cutting of the dough into wedges.  Others link it to the Dutch word \"schoonbrood,\" meaning \"fine bread\" or \"sweet bread.\" Regardless of the etymology, they're undeniably a delightful treat.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "b73a2d67",
   "metadata": {},
   "source": [
    "As you can see, the model successfully understood the context of the image and gave us useful information. This showcases how to use the Gemini API for multimodal inputs. \n",
    "\n",
    "Now let's move on to system prompts!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d79a5",
   "metadata": {},
   "source": [
    "### 2. Using system prompts\n",
    "System prompts allow you to set the context and behaviour of the model. They are useful for creating specific personas or setting specific guidelines for the model to follow. Add below a system instruction when instantiating the model.\n",
    "\n",
    "Take a look at the [documentation](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instructions) for some tips on writing system instructions."
   ]
  },
  {
   "cell_type": "code",
   "id": "0a0465b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:13:45.449414Z",
     "start_time": "2025-03-05T12:13:44.655318Z"
    }
   },
   "source": [
    "system_prompt = \"You are a helpful and concise assistant that summarizes the input in three bullet points.\"\n",
    "prompt = \"The French Revolution was a period of social and political upheaval in late 1700's France. It eventually toppled the monarchy and led to the rise of Napoleon Bonaparte.\"\n",
    "\n",
    "# YOUR CODE HERE START\n",
    "model = GenerativeModel(\"gemini-1.5-flash-002\", system_instruction = [system_prompt])\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "response = model.generate_content(\n",
    "    prompt,    \n",
    ")\n",
    "\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* The French Revolution was a period of radical social and political change in late 18th-century France.\n",
      "* It resulted in the overthrow of the French monarchy.\n",
      "* The Revolution ultimately paved the way for Napoleon Bonaparte's rise to power.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "9272e17d",
   "metadata": {},
   "source": [
    "As you can see, the model followed the system prompt and returned the summary in three bullet points. System prompts are a powerful way to control the model's behavior and to tailor it to your specific needs. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4607e",
   "metadata": {},
   "source": [
    "### 3. Using content generation parameters\n",
    "The Gemini API allows you to control the generation of text using several parameters such as `temperature`, `top_p` and `top_k`. These parameters affect the randomness and diversity of the generated content. Let's see what each of the do:\n",
    "\n",
    "*   `temperature`: Controls the randomness of the output. Higher values result in more random outputs and lower values will result in more deterministic outputs. The suggested range is 0.0 to 1.0 but higher values can also be used (up to 2.0).\n",
    "*   `top_p`: Controls the cumulative probability of the tokens selected. Lower values tend to lead to more focused generation, while higher values can result in more diverse outputs.\n",
    "*   `top_k`: Controls the number of tokens that can be selected. Higher values lead to more tokens being considered.\n",
    "\n",
    "Let's explore how temperature affects the model's output."
   ]
  },
  {
   "cell_type": "code",
   "id": "3225c937",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:15:52.887228Z",
     "start_time": "2025-03-05T12:15:51.586243Z"
    }
   },
   "source": [
    "prompt = \"Tell me something about the sky.\"\n",
    "print(\"Temperature 0.2\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config = {\n",
    "        # YOUR CODE HERE START\n",
    "        \"temperature\": 0.2\n",
    "        # YOUR CODE HERE END\n",
    "    }\n",
    ")\n",
    "print(response.text)\n",
    "\n",
    "print(\"\\nTemperature 1.0\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    # YOUR CODE HERE START\n",
    "    generation_config = {\"temperature\": 1.0}\n",
    "    # YOUR CODE HERE END\n",
    ")\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.2\n",
      "* The sky appears blue due to Rayleigh scattering of sunlight.\n",
      "* The sky's color changes throughout the day due to the sun's angle and atmospheric conditions.\n",
      "* The sky is not actually a physical object, but rather the appearance of the atmosphere.\n",
      "\n",
      "\n",
      "Temperature 1.0\n",
      "* The sky appears blue due to Rayleigh scattering of sunlight.\n",
      "* The sky's color changes depending on the time of day and weather conditions.\n",
      "* The sky is not actually a physical thing but an atmospheric perspective.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:18:13.584538Z",
     "start_time": "2025-03-05T12:18:12.329646Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Tell me something about the sky.\"\n",
    "print(\"Temperature 0.2\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config = {\n",
    "        # YOUR CODE HERE START\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_k\": 1\n",
    "        # YOUR CODE HERE END\n",
    "    }\n",
    ")\n",
    "print(response.text)\n",
    "\n",
    "print(\"\\nTemperature 1.0\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    # YOUR CODE HERE START\n",
    "    generation_config = {\"temperature\": 1.0, \"top_k\": 1}\n",
    "    # YOUR CODE HERE END\n",
    ")\n",
    "print(response.text)"
   ],
   "id": "413fe154cede4c31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.2\n",
      "* The sky appears blue due to Rayleigh scattering of sunlight.\n",
      "* It's a vast expanse containing the atmosphere, weather systems, and celestial bodies.\n",
      "* Its color changes throughout the day due to the sun's position and atmospheric conditions.\n",
      "\n",
      "\n",
      "Temperature 1.0\n",
      "* The sky appears blue due to Rayleigh scattering of sunlight.\n",
      "* It is a vast expanse containing the atmosphere, clouds, and celestial bodies.\n",
      "* The color of the sky changes throughout the day due to the angle of the sun.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:19:19.014224Z",
     "start_time": "2025-03-05T12:19:17.485858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "prompt = \"Tell me something about the sky.\"\n",
    "print(\"Temperature 0.2\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    generation_config = {\n",
    "        # YOUR CODE HERE START\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.3\n",
    "        # YOUR CODE HERE END\n",
    "    }\n",
    ")\n",
    "print(response.text)\n",
    "\n",
    "print(\"\\nTemperature 1.0\")\n",
    "response = model.generate_content(\n",
    "    prompt,\n",
    "    # YOUR CODE HERE START\n",
    "    generation_config = {\"temperature\": 1.0, \"top_p\": 0.9}\n",
    "    # YOUR CODE HERE END\n",
    ")\n",
    "print(response.text)"
   ],
   "id": "4948accbfd27b8ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.2\n",
      "* The sky appears blue due to Rayleigh scattering of sunlight.\n",
      "* The sky's color varies depending on the time of day and weather conditions.\n",
      "* The sky is not actually a physical object but rather the atmosphere as seen from the Earth's surface.\n",
      "\n",
      "\n",
      "Temperature 1.0\n",
      "* The sky appears blue due to Rayleigh scattering of sunlight.\n",
      "* The sky's color changes depending on the time of day and weather conditions.\n",
      "* The sky is not actually a solid object; it's the atmosphere surrounding the Earth.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "bb2f440e",
   "metadata": {},
   "source": [
    "Try also re-running the cell several times to see how random the output of the model is. As you can see, the output of the model changes based on the temperature. A lower temperature makes the output more deterministic and focused, while a higher temperature makes the output more creative and diverse.\n",
    "\n",
    "You can experiment with the `top_p` and `top_k` parameters as well. \n",
    "\n",
    "This concludes this introduction into the Google Gemini Vertex AI API. You've learned how to connect to the API, generate text and process multimodal content, control the behaviour of the models with system prompts and control the output generation using temperature.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61c33f",
   "metadata": {},
   "source": [
    "## Optional: Exercise 3b - base model vs instruction fine-tuned model\n",
    "Let's make a CLI-based version of Gemini.\n",
    "\n",
    "Below, you will find some boilerplate code that takes care of asking the user for input and printing the conversation.\n",
    "However, there are still a few things missing like:\n",
    "- The content of the inital system prompt.\n",
    "- The API call to the model, which leads to a response from the chatbot based on the conversation so far.\n",
    "\n",
    "\n",
    "Can you finish it?\n",
    "\n",
    "<mark>Note: The text box should appear at the top of your IDE (VS Code) window.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "id": "51666c3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T12:48:19.056877Z",
     "start_time": "2025-03-05T12:48:05.681231Z"
    }
   },
   "source": [
    "import time\n",
    "\n",
    "# The initial system prompt.\n",
    "messages = [\n",
    "    Content(role='user', parts=[\n",
    "        Part.from_text('Hi there'),\n",
    "    ]),\n",
    "\n",
    "    # pass context to the model, simulating a system prompt, by mimicking a model response.\n",
    "    Content(role='model', parts=[\n",
    "        Part.from_text(\n",
    "            'Hi! I am an AI assistant that helps people with their every day tasks. How can I help you today?')\n",
    "    ])\n",
    "]\n",
    "\n",
    "chat = model.start_chat(history=messages)\n",
    "\n",
    "while True:\n",
    "    # Get the user's input.\n",
    "    user_message = input('User:').strip()\n",
    "\n",
    "    # Check whether to continue.\n",
    "    if len(user_message) == 0 or user_message == \"exit\":\n",
    "        print(\"exiting...\")\n",
    "        break\n",
    "    print(1)\n",
    "\n",
    "    # Send the user's message to the model.\n",
    "    # YOUR CODE HERE START\n",
    "    response = chat.send_message(user_message,\n",
    "                                 generation_config={\"temperature\": 1.0, \"top_k\": 5})\n",
    "    # YOUR CODE HERE END\n",
    "\n",
    "    assistant_message = response.text\n",
    "    print(2)\n",
    "\n",
    "    # Print the user input and response.\n",
    "    print(\"User:\", user_message)\n",
    "    print(\"AI:\", assistant_message)\n",
    "\n",
    "    # We need to wait a little bit of time for the text to render.\n",
    "    time.sleep(0.5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "User: I am looking for a recipe to cook tonight. I have spaghetti, eggs, chicken breast, cheese, and basil. What should I make for dinner?\n",
      "AI: * **Creamy Chicken Spaghetti:** Use the chicken breast to make a creamy chicken sauce, toss with spaghetti, and top with cheese and basil.\n",
      "* **Chicken and Egg Stir-fry with Spaghetti:** Stir-fry diced chicken and scrambled eggs, then mix with cooked spaghetti and sprinkle with cheese and basil.\n",
      "* **Simple Chicken Spaghetti:**  Cook the chicken and create a simple sauce using some of the pasta water and cheese.  Toss with spaghetti and garnish with basil.\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 20\u001B[0m\n\u001B[1;32m     16\u001B[0m chat \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mstart_chat(history\u001B[38;5;241m=\u001B[39mmessages)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m     19\u001B[0m     \u001B[38;5;66;03m# Get the user's input.\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m     user_message \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mUser:\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstrip()\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# Check whether to continue.\u001B[39;00m\n\u001B[1;32m     23\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(user_message) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m user_message \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexit\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Projects/bol/training/academy-llm-applications-course/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001B[0m, in \u001B[0;36mKernel.raw_input\u001B[0;34m(self, prompt)\u001B[0m\n\u001B[1;32m   1280\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw_input was called, but this frontend does not support input requests.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1281\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m StdinNotImplementedError(msg)\n\u001B[0;32m-> 1282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_input_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1283\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1284\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_parent_ident\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1285\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_parent\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mshell\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpassword\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1287\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Projects/bol/training/academy-llm-applications-course/.venv/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001B[0m, in \u001B[0;36mKernel._input_request\u001B[0;34m(self, prompt, ident, parent, password)\u001B[0m\n\u001B[1;32m   1322\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m:\n\u001B[1;32m   1323\u001B[0m     \u001B[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001B[39;00m\n\u001B[1;32m   1324\u001B[0m     msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInterrupted by user\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1325\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyboardInterrupt\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1326\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m   1327\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlog\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInvalid Message:\u001B[39m\u001B[38;5;124m\"\u001B[39m, exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: Interrupted by user"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "ba08ff8b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
