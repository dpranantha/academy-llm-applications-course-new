{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a61ebce70e82a34b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 1 - Tokenization\n",
    "In this exercise, we will explore how text is tokenized. The goal is to develop a deeper understanding of the tokenization process and how it varies across different languages.\n",
    "\n",
    "## Tokenization App\n",
    "\n",
    "To see an example, OpenAI provides a clear demonstration of how an application like ChatGPT performs tokenization. You can see the demonstration [here](https://platform.openai.com/tokenizer).\n",
    "\n",
    "![](../../assets/openai-tokenizer.png)\n",
    "\n",
    "## Exercise 1a\n",
    "Using the tool linked above, try to answer the following questions:\n",
    "- What are the characteristics of the English words that do not get split into subtokens? (e.g., length, type of word, rareness, etc.)\n",
    "- What are the characteristics of the English words that get split into subtokens? (e.g., length, type of word, rareness, etc.)\n",
    "\n",
    "After you have examined English words, try to do the same for non-English words. Ideally, use a non-English language you know well. If you do not know any non-English languages, you can use Google Translate to translate some English text. Try to answer the following questions:\n",
    "- What are the characteristics of the non-English words that do not get split into subtokens? (e.g., length, type of word, rareness, etc.)\n",
    "- What are the characteristics of the non-English words that do get split into subtokens? (e.g., length, type of word, rareness, etc.)\n",
    "- Do you notice any differences between the English and non-English tokens? (e.g., number of tokens used, average length of tokens, etc.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34fd5c019491377f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tokenization with Python\n",
    "\n",
    "The tokenizer used in the visualization tool is also available as a Python SDK through the [tiktoken](https://github.com/openai/tiktoken) package.\n",
    "\n",
    "In this exercise, we'll use **tiktoken** to gain a deeper understanding of how tokenization works.\n",
    "\n",
    "Note: While this exercise uses an OpenAI tokenization library, the process is similar across various large language models (LLMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72239939e2a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tiktoken import encoding_for_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1416e82039311124",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Next, we need to select which tokenizer we want to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29862ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_tokenizer = encoding_for_model(\"gpt-4\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c625dea23e7e8d5a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now use the tokenizer as follows:\n",
    "- `openai_tokenizer.encode(text)` will return the token indices of the text.\n",
    "- `openai_tokenizer.decode_single_token_bytes(token)` will return the token index in text format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970732a3b358c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hello world or hello-world\"\n",
    "token_indices = openai_tokenizer.encode(text)\n",
    "tokens = [openai_tokenizer.decode_single_token_bytes(token).decode('utf-8') for token in token_indices]\n",
    "print(\"These are the indices of the tokens:\", token_indices)\n",
    "print(\"These are the tokens in text format:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c0da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Voici une phrase en Fran√ßais.\"\n",
    "token_indices = openai_tokenizer.encode(text)\n",
    "tokens = [openai_tokenizer.decode_single_token_bytes(token).decode('utf-8') for token in token_indices]\n",
    "print(\"These are the indices of the tokens:\", token_indices)\n",
    "print(\"These are the tokens in text format:\", tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f27fd2638b34d9aa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 1b - Tokenizing different languages\n",
    "Next, we will use the tokenizer to calculate the average token length of an English text and a non-English text.\n",
    "We have given you two example texts, one in English and one in Dutch. You can use these texts or replace them with your own texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09c7003cc66d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"A windmill is a structure that converts wind power into rotational energy using vanes called sails or blades.\"\n",
    "\n",
    "# YOUR CODE HERE START\n",
    "token_indices = openai_tokenizer.encode(text)\n",
    "\n",
    "token_lengths = []\n",
    "for token_idx in token_indices:\n",
    "    token = openai_tokenizer.decode_single_token_bytes(token_idx).decode('utf-8')\n",
    "    token_lengths.append(len(token))\n",
    "    \n",
    "print(\"Average token length:\", sum(token_lengths) / len(token_lengths))\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e668f3992015b2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Een windmolen is een constructie die windenergie omzet in rotatie-energie met behulp van schoepen die zeilen of bladen worden genoemd.\"\n",
    "\n",
    "# YOUR CODE HERE START\n",
    "token_indices = openai_tokenizer.encode(text)\n",
    "\n",
    "token_lengths = []\n",
    "for token_idx in token_indices:\n",
    "    token = openai_tokenizer.decode_single_token_bytes(token_idx).decode('utf-8')\n",
    "    token_lengths.append(len(token))\n",
    "    \n",
    "print(\"Average token length:\", sum(token_lengths) / len(token_lengths))\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18964503",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
