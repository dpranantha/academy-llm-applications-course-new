{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Apps with Streamlit\n",
    "\n",
    "Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science.\n",
    "\n",
    "We will use streamlit to build our LLM applications, but first, let's take a look at how to use it.\n",
    "\n",
    "![](images/streamlit.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to run a Streamlit App\n",
    "\n",
    "You'll need to save the code for your app in a `.py` Python file.\n",
    "\n",
    "Then to run a Streamlit app, use the following command in your terminal:\n",
    "\n",
    "```\n",
    "streamlit run path/to/your_script.py\n",
    "```\n",
    "\n",
    "Below are some examples demonstrating Streamlit features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple streamlit examples\n",
    "\n",
    "### Example 1: Simple Line Chart\n",
    "\n",
    "This code below creates an App that shows a simple line chart of a sine wave.\n",
    "\n",
    "Copy and paste this code to a new file called `simple_line_chart.py`. Remember to save the file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile simple_line_chart.py\n",
    "\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Our First Streamlit App\")\n",
    "st.write(\"The description of our app, or some explanatory text.\")\n",
    "\n",
    "x = np.arange(0, 2 * np.pi, 0.01)\n",
    "y = np.sin(x)\n",
    "st.line_chart(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in your **terminal** to start the App:\n",
    "\n",
    "```python\n",
    "uv run streamlit run path/to/simple_line_chart.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we have written a helper function to save you typing the path to your file.\n",
    "\n",
    "```python\n",
    "uv run invoke simple-line-chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Interactive Line Chart\n",
    "In this example, we add interactivity using sliders to adjust the amplitude and period of the sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile interaction.py\n",
    "\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Interactive Line Chart\")\n",
    "st.write(\"Adjust the sliders to modify the sine wave.\")\n",
    "\n",
    "amplitude = st.slider(\"Amplitude\", min_value=0, max_value=10, value=1)\n",
    "period = st.slider(\"Period\", min_value=0, max_value=10, value=1)\n",
    "\n",
    "st.write(f\"This is a sine wave with amplitude **{amplitude}** and period **{period}**.\")\n",
    "\n",
    "x = np.arange(0, 2 * np.pi, 0.01) * period\n",
    "y = amplitude * np.sin(x)\n",
    "st.line_chart(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating `interaction.py`, run the following command in your terminal to open the App.\n",
    "\n",
    "```python\n",
    "uv run invoke interaction\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Using a Sidebar\n",
    "This example moves the sliders into a sidebar to keep the main interface cleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile sidebar.py\n",
    " \n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "\n",
    "st.title(\"Sidebar Controls\")\n",
    "st.write(\"Use the sidebar sliders to modify the sine wave.\")\n",
    "\n",
    "st.sidebar.markdown(\"# Controls\")\n",
    "amplitude = st.sidebar.slider(\"Amplitude\", min_value=0, max_value=10, value=1)\n",
    "period = st.sidebar.slider(\"Period\", min_value=0, max_value=10, value=1)\n",
    "\n",
    "st.write(f\"This is a sine wave with amplitude **{amplitude}** and period **{period}**.\")\n",
    "\n",
    "x = np.arange(0, 2 * np.pi, 0.01)\n",
    "y = amplitude * np.sin(x * period)\n",
    "st.line_chart(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating `sidebar.py`, run the following command in your terminal to open the App.\n",
    "\n",
    "```python\n",
    "uv run invoke sidebar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to start making apps using the **LangChain** API.\n",
    "\n",
    "## Introduction to LangChain\n",
    "\n",
    "In the previous notebook, we explored how to interact directly with language models, like OpenAI's GPT or Google's Gemini (Vertex AI). While these APIs are powerful, working with them directly can become repetitive and cloud-specific.\n",
    "\n",
    "LangChain is a Python-based framework that simplifies the use of language models, providing a cloud-agnostic interface for interacting with providers like Google Vertex AI and (Azure) OpenAI. With LangChain, you can focus on what you want the model to do, not how to interact with different APIs.\n",
    "\n",
    "**Why Use LangChain?**\n",
    "- Cloud-Agnostic: Switch between providers (e.g., OpenAI, Azure, Google) with minimal changes to your code.\n",
    "- Simplified Usage: LangChain abstracts away API-specific details, letting you use models in a more consistent and straightforward way.\n",
    "- Scalability: As your projects grow, LangChain makes it easier to manage interactions with models.\n",
    "\n",
    "\n",
    "Let's import the necessary libraries and create a client for our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.llm import instantiate_langchain_model\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Here we create the client. \n",
    "# Make sure you select the LLM provider that corresponds to the one you are using in this course!\n",
    "client = instantiate_langchain_model(\n",
    "    # llm_provider=\"azure\",\n",
    "    llm_provider=\"gcp\",\n",
    ")\n",
    "client.model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the LangChain client, you can call the `invoke` method. This takes a prompt as input and generates text.\n",
    "\n",
    "The prompt can be a simple **instruction** to follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke(\"Hello, tell me about building Apps with streamlit.\")\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, you can structure your prompt with `role`s.\n",
    "\n",
    "- `system`: This contains the system instructions the model should follow during the conversation.\n",
    "- `user`:  This means that the message is something the user said.\n",
    "- `assistant`: This means that the message is something the assistant/model said."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You explain Python concepts in a simple and clear manner.\"\n",
    "user_message = \"Can you explain how to build streamlit apps?\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": system_prompt},\n",
    "           {\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "\n",
    "response = client.invoke(\n",
    "    input=messages,\n",
    "    temperature=1.0)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: A Streamlit Chat App\n",
    "\n",
    "Streamlit has great [in-built functionality for building chat apps](https://docs.streamlit.io/develop/tutorials/chat-and-llm-apps/build-conversational-apps).\n",
    "\n",
    "The example below can be used to create a very simple chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile simple_chatbot.py\n",
    "\n",
    "import streamlit as st\n",
    "from llm_in_production.llm import instantiate_langchain_model\n",
    "import dotenv\n",
    "\n",
    "# Step 1: Set up Streamlit UI\n",
    "title = \"Simple Chatbot\"\n",
    "st.set_page_config(\n",
    "    page_title=title,\n",
    "    page_icon=\"ðŸ‘‹\",\n",
    ")\n",
    "st.title(title)\n",
    "\n",
    "# Step 2: Create LLM Client\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Here we create the client. \n",
    "# Make sure you select the LLM provider that corresponds to the one you are using in this course!\n",
    "client = instantiate_langchain_model(\n",
    "    # llm_provider=\"azure\",\n",
    "    llm_provider=\"gcp\",\n",
    ")\n",
    "st.write(\"Powered by\", client.model_name)\n",
    "\n",
    "\n",
    "# Step 3: Initialize Chat History\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "# Step 4: Display Chat History\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# Step 5: Accept User Input\n",
    "prompt = st.chat_input(\"Say something:\")\n",
    "\n",
    "if prompt:\n",
    "    # Add user message to chat history\n",
    "    input_message = {\"role\": \"user\", \"content\": prompt}\n",
    "    st.session_state.messages.append(input_message)\n",
    "\n",
    "    # Display user message\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "\n",
    "    # Simple response logic\n",
    "    response_message = client.invoke(st.session_state.messages).content\n",
    "    \n",
    "    # Display chatbot response\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response_message)\n",
    "    \n",
    "    # Store assistant response\n",
    "    st.session_state.messages.append(\n",
    "        {\"role\": \"assistant\", \"content\": response_message}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating `simple_chatbot.py`, run the following command in your terminal to open the App.\n",
    "\n",
    "```python\n",
    "uv run invoke simple-chatbot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streamlit LLM Apps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that you know Streamlit and LangChain, you've all the tools you need to build your own LLM Chatbot App!\n",
    "\n",
    "Run the following command in your terminal to get started!\n",
    "\n",
    "```python\n",
    "uv run invoke chatot\n",
    "```\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
