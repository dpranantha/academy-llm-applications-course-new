{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5 - Guardrails\n",
    "\n",
    "A guardrail is a generic term for detective controls that aim to steer your application. \n",
    "\n",
    "Greater steerability is a common requirement given the inherent randomness of LLMs, and so creating effective guardrails has become one of the most common areas of performance optimization when pushing an LLM from prototype to production.\n",
    "\n",
    "Note: This notebook tackles guardrails as a generic term for detective controls around an LLM - for the official libraries that provide distributions of pre-built guardrails frameworks, please check out the following:\n",
    "\n",
    "- [NeMo Guardrails](https://github.com/NVIDIA/NeMo-Guardrails/tree/main)\n",
    "- [Guardrails AI](https://github.com/guardrails-ai/guardrails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.openai_utils import get_openai_client\n",
    "import dotenv\n",
    "import os\n",
    "import asyncio\n",
    "\n",
    "# This reads the .env file in your project and transforms its content into env variables.\n",
    "# This way you don't have to hard code your secrets.\n",
    "dotenv.load_dotenv()\n",
    "# Here we create the client.\n",
    "client = get_openai_client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Types of Guardrails\n",
    "\n",
    "Guardrails are incredibly diverse and can be deployed to virtually any context you can imagine something going wrong with LLMs. \n",
    "\n",
    "In general, they can be categorised as follows:\n",
    "\n",
    "1. **Input guardrails** that flag inappropriate content before it gets to your LLM\n",
    "\n",
    "2. **Output guardrails** that validate what your LLM has produced before it gets to the customer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tradeoffs\n",
    "\n",
    "When designing guardrails it is important to consider the trade-off between **accuracy**, **latency** and **cost**, where you try to achieve maximum accuracy for the least impact to your bottom line and the user's experience.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations\n",
    "\n",
    "You should always consider the limitations of guardrails when developing your design. A few of the key ones to be aware of are:\n",
    "\n",
    "- When using LLMs as a guardrail, be aware that they have the same vulnerabilities as your base LLM call itself. For example, a **prompt injection** attempt could be successful in evading both your guardrail and your actual LLM call.\n",
    "- As conversations get longer, LLMs are more susceptible to **jailbreaking** as your instructions become diluted by the extra text.\n",
    "- Guardrails can harm the user experience if you make them overly restrictive to compensate for the issues noted above. This manifests as **over-refusals**, where your guardrails reject innocuous user requests because there are similarities with prompt injection or jailbreaking attempts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mitigations\n",
    "\n",
    "If you can combine guardrails with rules-based or more traditional machine learning models for detection this can mitigate some of these risks. We've also seen customers have guardrails that only ever consider the latest message, to alleviate the risks of the model being confused by a long conversation.\n",
    "\n",
    "We would also recommend doing a gradual roll-out with active monitoring of conversations so you can pick up instances of prompt injection or jailbreaking, and either add more guardrails to cover these new types of behaviour, or include them as training examples to your existing guardrails."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Guardrails\n",
    "\n",
    "Input guardrails aim to prevent inappropriate content getting to the LLM in the first place - some common use cases are:\n",
    "\n",
    "- **Topical guardrails**: Identify when a user asks an off-topic question and give them advice on what topics the LLM can help them with.\n",
    "- **Jailbreaking**: Detect when a user is trying to hijack the LLM and override its prompting.\n",
    "- **Prompt injection**: Pick up instances of prompt injection where users try to hide malicious code that will be executed in any downstream functions the LLM executes.\n",
    "\n",
    "In all of these they act as a preventative control, running either before or in parallel with the LLM, and triggering your application to behave differently if one of these criteria are met.\n",
    "\n",
    "### Topic Guardrails\n",
    "\n",
    "Let's create a **topic guardrail**, which uses gpt-3.5-turbo to detect off-topic questions and prevent the LLM from answering if triggered. This guardrail prioritises correctness over cost/latency.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `async`\n",
    "A common design to minimize latency is to send your guardrails asynchronously along with your main LLM call. If your guardrails get triggered you send back their response, otherwise send back the LLM response.\n",
    "\n",
    "We'll use this approach, creating an execute_chat_with_guardrails function that will run our LLM's get_chat_response and the topical_guardrail guardrail in parallel, and return the LLM response only if the guardrail returns allowed.\n",
    "\n",
    "We'll use this approach, creating an `execute_chat_with_guardrails` function that will run our LLM's `get_chat_response` and the `topical_guardrail` guardrail in parallel, and return the LLM response only if the guardrail returns `allowed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "good_request = \"What are the best breeds of dog for people that like cats?\"\n",
    "bad_request = \"I want to talk about horses\"\n",
    "\n",
    "async def get_chat_response(user_request, system_prompt):\n",
    "    print(\"Getting LLM response\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_request},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages, \n",
    "        model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    "        temperature=0.5\n",
    "    )\n",
    "    print(\"Got LLM response\")\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topical_system_prompt = \"Your role is to assess whether the user question is allowed or not. The allowed topics are cats and dogs. If the topic is allowed, say 'allowed' otherwise say 'not_allowed'\"\n",
    "\n",
    "async def topical_guardrail(user_request, topical_system_prompt):\n",
    "    print(\"Checking topical guardrail\")\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": topical_system_prompt,        },\n",
    "        {\"role\": \"user\", \"content\": user_request},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        messages=messages, \n",
    "        model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    "        temperature=0\n",
    "    )\n",
    "\n",
    "    print(\"Got guardrail response\")\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topical_guardrail_message = \"I can only talk about cats and dogs.\"\n",
    "\n",
    "async def execute_topical_guardrail(user_request, system_prompt, topical_system_prompt, topical_guardrail_message):\n",
    "    chat_task = asyncio.create_task(get_chat_response(user_request, system_prompt))\n",
    "    topical_guardrail_task = asyncio.create_task(topical_guardrail(user_request, topical_system_prompt))\n",
    "\n",
    "    while True:\n",
    "        done, _ = await asyncio.wait(\n",
    "            [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n",
    "        )\n",
    "        if topical_guardrail_task in done:\n",
    "            guardrail_response = topical_guardrail_task.result()\n",
    "            if guardrail_response == \"not_allowed\":\n",
    "                chat_task.cancel()\n",
    "                print(\"Topical guardrail triggered\")\n",
    "                return topical_guardrail_message\n",
    "            elif chat_task in done:\n",
    "                chat_response = chat_task.result()\n",
    "                return chat_response\n",
    "        else:\n",
    "            await asyncio.sleep(0.1)  # sleep for a bit before checking the tasks again"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the main function with the good request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await execute_topical_guardrail(good_request, system_prompt, topical_system_prompt, topical_guardrail_message)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the bad request. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await execute_topical_guardrail(bad_request, system_prompt, topical_system_prompt, topical_guardrail_message)\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our guardrail worked - the first question was allowed through, but the second was blocked for being off-topic. Now we'll extend this concept to moderate the response we get from the LLM as well."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Guardrails\n",
    "\n",
    "Output guardrails govern what the LLM comes back with. These can take many forms, with some of the most common being:\n",
    "\n",
    "- **Hallucination/fact-checking guardrails**: Using a corpus of ground truth information or a training set of hallucinated responses to block hallucinated responses.\n",
    "- **Moderation guardrails**: Applying brand and corporate guidelines to moderate the LLM's results, and either blocking or rewriting its response if it breaches them.\n",
    "- **Syntax checks**: Structured outputs from LLMs can be returned corrupt or unable to be parsed - these guardrails detect those and either retry or fail gracefully, preventing failures in downstream applications. This is a common control to apply with function/tool calling LLMs, as will be discussed later. \n",
    "\n",
    "### Moderation Guardrails\n",
    "\n",
    "Here we implement a **moderation guardrail** that uses a version of the [G-Eval](https://arxiv.org/abs/2303.16634) evaluation method to score the presence of unwanted content in the LLM's response. This method is demonstrated in more detail [here](https://github.com/openai/openai-cookbook/blob/main/examples/evaluation/How_to_eval_abstractive_summarization.ipynb).\n",
    "\n",
    "To accomplish this we will make an extensible framework for moderating content:\n",
    "\n",
    "1. We set a domain name, which describes the type of content we're going to moderate.\n",
    "2. We provide criteria, which outline clearly what the content should and should not contain.\n",
    "3. Step-by-step instructions are provided for the LLM to grade the content.\n",
    "4. The LLM returns a discrete score from 1-5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"animal breed recommendation\"\n",
    "\n",
    "criteria = \"\"\"\n",
    "Assess the presence of explicit recommendation of cat or dog breeds in the content.\n",
    "The content should contain only general advice about cats and dogs, not specific breeds to purchase.\"\"\"\n",
    "\n",
    "steps = \"\"\"\n",
    "1. Read the content and the criteria carefully.\n",
    "2. Assess how much explicit recommendation of cat or dog breeds is contained in the content.\n",
    "3. Assign an animal advice score from 1 to 5, with 1 being no explicit cat or dog breed advice, and 5 being multiple named cat or dog breeds.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "moderation_system_prompt = \"\"\"\n",
    "You are a moderation assistant. Your role is to detect content about {domain} in the text provided, and mark the severity of that content.\n",
    "\n",
    "## {domain}\n",
    "\n",
    "### Criteria\n",
    "\n",
    "{scoring_criteria}\n",
    "\n",
    "### Instructions\n",
    "\n",
    "{scoring_steps}\n",
    "\n",
    "### Content\n",
    "\n",
    "{content}\n",
    "\n",
    "### Evaluation (score only!)\n",
    "\"\"\"\n",
    "\n",
    "async def moderation_guardrail(chat_response, moderation_system_prompt):\n",
    "    print(\"Checking moderation guardrail\")\n",
    "    mod_messages = [\n",
    "        {\"role\": \"user\", \"content\": moderation_system_prompt.format(\n",
    "            domain=domain,\n",
    "            scoring_criteria=criteria,\n",
    "            scoring_steps=steps,\n",
    "            content=chat_response\n",
    "        )},\n",
    "    ]\n",
    "    response = client.chat.completions.create(\n",
    "        messages=mod_messages, \n",
    "        model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    "        temperature=0\n",
    "    )\n",
    "    print(\"Got moderation response\")\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_guardrail_message = \"Sorry, we're not permitted to give animal breed advice. I can help you with any general queries you might have.\"\n",
    "\n",
    "async def execute_moderation_guardrail(user_request, system_prompt, moderation_system_prompt, moderation_guardrail_message):\n",
    "    chat_task = asyncio.create_task(get_chat_response(user_request, system_prompt))\n",
    "\n",
    "    while True:\n",
    "        done, _ = await asyncio.wait(\n",
    "            [chat_task], return_when=asyncio.FIRST_COMPLETED\n",
    "        )\n",
    "        if chat_task in done:\n",
    "            chat_response = chat_task.result()\n",
    "            moderation_response = await moderation_guardrail(chat_response, moderation_system_prompt)\n",
    "\n",
    "            if int(moderation_response) >= 3:\n",
    "                print(f\"Moderation guardrail flagged (with a score of {int(moderation_response)})\")\n",
    "                return moderation_guardrail_message\n",
    "            else:\n",
    "                print(f\"Passed moderation (with a score of {int(moderation_response)})\")\n",
    "                return chat_response\n",
    "        else:\n",
    "            await asyncio.sleep(0.1)  # sleep for a bit before checking the tasks again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a request that should pass the moderation guardrail\n",
    "great_request = 'What is some advice you can give to a new dog owner?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that without the topical guardrail, if the model response does not trigger the moderation guardrail, off-topic questions will still be answered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [good_request, great_request, bad_request]\n",
    "\n",
    "for test in tests:\n",
    "    result = await execute_moderation_guardrail(test, system_prompt, moderation_system_prompt, moderation_guardrail_message)\n",
    "    print(result)\n",
    "    print('\\n\\n')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Setting guardrail thresholds\n",
    "\n",
    "Our output guardrail assesses the LLM's response and block anything scoring *a 3 or higher*. Setting this threshold is a common area for optimization - we recommend building an evaluation set and grading the results using a confusion matrix to set the right tolerance for your guardrail. The trade-off here is generally:\n",
    "\n",
    "- More false positives leads to a fractured user experience, where customers get annoyed and the assistant seems less helpful.\n",
    "- More false negatives can cause lasting harm to your business, as people get the assistant to answer inappropriate questions, or prompt inject/jailbreak it.\n",
    "\n",
    "For example, for jailbreaking you may want to have a very low threshold, as the risk to your business if your LLM is hijacked and used to produce dangerous content that ends up on social media is very high. However, for our use case we're willing to accept a few false negatives, as the worst that could happen is someone ends up with a Bichon Frise who might have been better suited to a Labrador, which though sad will probably not cause lasting damage to our business (we hope)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5a\n",
    "\n",
    "\n",
    "Your organization has been tasked by the English Premier League to create a chatbot that will be used by players of their official Fantasy Football game to answer questions about the teams and players.\n",
    "\n",
    "However, the Premier League is concerned that people might ask the chatbot for advice about betting.\n",
    "\n",
    "It is your job to implement guardrails for a prototype that:\n",
    "\n",
    "1. Keep the chatbot focused on the topic of the English Premier League.\n",
    "2. Ensure responses don't contain any advice regarding betting or gambling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a helpful assistant that answers question to do with the English Premier League.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a helpful function for cleaning your answer prompts and guardrail messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_answer(text):\n",
    "    # Define the substrings to remove\n",
    "    start_substring = \"\\n# YOUR CODE HERE START\"\n",
    "    end_substring = \"\\n# YOUR CODE HERE END\"\n",
    "\n",
    "    # Remove substrings from text\n",
    "    return text.replace(start_substring, \"\").replace(end_substring, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise 5ai\n",
    "\n",
    "Write a topical system prompt and guardrail message to ensure it only answers on-topic questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topical_system_prompt = \"\"\"\n",
    "# YOUR CODE HERE START\n",
    "Your role is to assess whether the user question is allowed or not. The allowed topic is anything to do with the English Premier League.\n",
    "\n",
    "For example, its history, teams, managers, players and Fantasy Football game. \n",
    "\n",
    "If the topic is allowed, say 'allowed' otherwise say 'not_allowed'\n",
    "# YOUR CODE HERE END\n",
    "\"\"\"\n",
    "\n",
    "topical_guardrail_message = \"\"\"\n",
    "# YOUR CODE HERE START\n",
    "I can only talk about the English Premier League.\n",
    "# YOUR CODE HERE END\n",
    "\"\"\"\n",
    "\n",
    "# Clean answers\n",
    "topical_system_prompt = clean_answer(topical_system_prompt)\n",
    "topical_guardrail_message = clean_answer(topical_guardrail_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5aii\n",
    "\n",
    "Write out the domain, criteria and steps to be used in the moderation system prompt, as well as the moderation guardrail message, to ensure responses do not contain any advice on betting/gambling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"\"\"\n",
    "# YOUR CODE HERE START\n",
    "sports betting\n",
    "# YOUR CODE HERE END\n",
    "\"\"\"\n",
    "\n",
    "criteria = \"\"\"\n",
    "# YOUR CODE HERE START\n",
    "Assess the presence of explicit mention of sports betting in the content.\n",
    "The content should contain information about the English Premier League and not provide any gambling advice.\n",
    "# YOUR CODE HERE END\n",
    "\"\"\"\n",
    "\n",
    "steps = \"\"\"\n",
    "# YOUR CODE HERE START\n",
    "1. Read the content and the criteria carefully.\n",
    "2. Assess how much explicit discussion of gambling is contained in the content.\n",
    "3. Assign a sports betting score from 1 to 5, with 1 being no explicit sports betting advice, and 5 being detailed sports betting advice.\n",
    "# YOUR CODE HERE END\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Clean answers\n",
    "domain = clean_answer(domain)\n",
    "criteria = clean_answer(criteria)\n",
    "steps = clean_answer(steps)\n",
    "\n",
    "# Check you moderation system prompt\n",
    "print(moderation_system_prompt.format(\n",
    "            domain=domain,\n",
    "            scoring_criteria=criteria,\n",
    "            scoring_steps=steps,\n",
    "            content=\"MODEL RESPONSE TO BE ADDED HERE\"\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moderation_guardrail_message = \"\"\"\n",
    "# YOUR CODE HERE START\n",
    "Sorry, we're not permitted to give gambling advice. I can help you with any queries you might have about Teams and Players in the Premier League though.\n",
    "# YOUR CODE HERE END\n",
    "\"\"\"\n",
    "\n",
    "moderation_guardrail_message = clean_answer(moderation_guardrail_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5ciii\n",
    "\n",
    "Write an `execute all guardrails` function that implements both topical and moderation guardrails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_all_guardrails(user_request, system_prompt, topical_system_prompt, topical_guardrail_message, moderation_system_prompt, moderation_guardrail_message):\n",
    "    chat_task = asyncio.create_task(get_chat_response(user_request, system_prompt))\n",
    "    # YOUR CODE HERE START\n",
    "    topical_guardrail_task = asyncio.create_task(topical_guardrail(user_request, topical_system_prompt))\n",
    "\n",
    "    while True:\n",
    "        done, _ = await asyncio.wait(\n",
    "            [topical_guardrail_task, chat_task], return_when=asyncio.FIRST_COMPLETED\n",
    "        )\n",
    "        if topical_guardrail_task in done:\n",
    "            guardrail_response = topical_guardrail_task.result()\n",
    "            if guardrail_response == \"not_allowed\":\n",
    "                chat_task.cancel()\n",
    "                print(\"Topical guardrail triggered\")\n",
    "                return topical_guardrail_message\n",
    "            elif chat_task in done:\n",
    "                chat_response = chat_task.result()\n",
    "                moderation_response = await moderation_guardrail(chat_response, moderation_system_prompt)\n",
    "\n",
    "                if int(moderation_response) >= 3:\n",
    "                    print(f\"Moderation guardrail flagged (with a score of {int(moderation_response)})\")\n",
    "                    return moderation_guardrail_message\n",
    "                else:\n",
    "                    print(f\"Passed moderation (with a score of {int(moderation_response)})\")\n",
    "                    return chat_response\n",
    "        else:\n",
    "            await asyncio.sleep(0.1)  # sleep for a bit before checking the tasks again\n",
    "    # YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_request = \"What is the last season you have information about? And who was the top scorer?\"\n",
    "offtopic_request = \"I want to talk about rugby\"\n",
    "gambling_request = \"Which players from Arsenal would be good to include in my fantasy team? Do you have any tips about betting on Premier League teams?\"\n",
    "borderline_request = \"Who was the sponsor of Fulham F.C. for the 20/21 season?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = [acceptable_request, offtopic_request, gambling_request, borderline_request]\n",
    "\n",
    "for test in tests:\n",
    "    result = await execute_all_guardrails(test, system_prompt, topical_system_prompt, topical_guardrail_message, moderation_system_prompt, moderation_guardrail_message)\n",
    "    print(result)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Guardrails are a vibrant and evolving topic in LLMs, and we hope this notebook has given you an effective introduction to the core concepts around guardrails. To recap:\n",
    "\n",
    "- Guardrails are detective controls that aim to prevent harmful content getting to your applications and your users, and add steerability to your LLM in production.\n",
    "- They can take the form of input guardrails, which target content before it gets to the LLM, and output guardrails, which control the LLM's response.\n",
    "- Designing guardrails and setting their thresholds is a trade-off between accuracy, latency, and cost. Your decision should be based on clear evaluations of the performance of your guardrails, and an understanding of what the cost of a false negative and false positive are for your business.\n",
    "- By embracing asynchronous design principles, you can scale guardrails horizontally to minimize the impact to the user as your guardrails increase in number and scope.\n",
    "\n",
    "We look forward to seeing how you take this forward, and how thinking on guardrails evolves as the ecosystem matures."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
