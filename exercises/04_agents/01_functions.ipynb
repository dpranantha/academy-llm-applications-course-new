{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Function calling with the OpenAI API"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large\u00a0Language\u00a0Models (LLMs) are incredibly powerful. But, as we have seen, they can struggle with simple mathematics problems and are limited to the information captured in the training data.\n",
    "\n",
    "To this end, the Chat Completion API now allows us to describe functions and tools to the model. The models themselves won't call the functions, but they can determine if they are needed and return the arguments to use.\n",
    "\n",
    "In this notebook you will learn how describing functions to a model can transform it into agent, which is able to reason with itself to answer a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload imports\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.openai_utils import get_openai_client\n",
    "from llm_in_production.agent_utils import (\n",
    "    function_to_json,\n",
    "    function_calling_agent,\n",
    ")\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tiktoken\n",
    "\n",
    "# This reads the .env file in your project and transforms its content into env variables.\n",
    "# This way you don't have to hard code your secrets.\n",
    "dotenv.load_dotenv()\n",
    "# Here we create the client.\n",
    "client = get_openai_client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM limiations\n",
    "\n",
    "Let's start by demonstrating one of the limitations of LLMs.\n",
    "\n",
    "The prompt below is asking the model to answer a simple highschool mathematics problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_problem = \"what is the circumference of a circle with radius 5.31cm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a calculator bot that is used to answer mathematics problems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": math_problem},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.environ[\"GPT_4_MODEL_NAME\"],\n",
    "    messages=messages,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "message = response.choices[0].message.content\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the answer it produces is incorrect.\n",
    "\n",
    "$2*\\pi*5.31 = 33.3637139811...$\n",
    "\n",
    "For this problem, a simple function would be more useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circumference_calculator(radius: float, something: float = 4.4) -> float:\n",
    "    \"\"\"Calculates the circumference of a circle given the radius\n",
    "\n",
    "    :param radius: The radius of the circle\n",
    "    :return: The circumference of the circle\n",
    "    \"\"\"\n",
    "    return 2 * np.pi * radius\n",
    "\n",
    "\n",
    "circumference_calculator(5.31)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if GPT knew about this function?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Calling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Chat Completion API allows us to describe functions and tools to the model. The model can then decide when it is appropriate to use one of these functions and if so, return the arguments for the function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe a function to the model it must be in a particular JSON format. We have ceated a helpful function to transform python functions into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circumference_calculator_json = function_to_json(circumference_calculator)\n",
    "circumference_calculator_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function described to the model, the response to the previous prompt is that the function should be called with the argument `radius=5.31`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": circumference_calculator_json}]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.environ[\"GPT_4_MODEL_NAME\"],\n",
    "    messages=messages,\n",
    "    seed=0,\n",
    "    tools=tools,\n",
    "    tool_choice = \"auto\"  # (default setting) the model will pick between generating a message or calling a function automatically \n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "message = response_message.content\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "print(f\"Message: {message}\")\n",
    "print(f\"Tool calls: {tool_calls}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do what the model suggests and call the function with the given argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_name = tool_calls[0].function.name\n",
    "function_args = json.loads(tool_calls[0].function.arguments)\n",
    "function_response = json.dumps(eval(function_name)(**function_args))\n",
    "\n",
    "print(f\"Fucntion response {function_response}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is correct. Let's pass information back to the model.\n",
    "\n",
    "First we need to update the message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response_message)\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": function_name,\n",
    "        \"content\": function_response,\n",
    "        \"tool_call_id\": tool_calls[0].id,\n",
    "    }\n",
    ")  # extend conversation with function response\n",
    "\n",
    "if len(messages) > 4:\n",
    "    raise Exception(\n",
    "        \"Too many messages have been added! Restart and rerun the notebook.\"\n",
    "    )\n",
    "\n",
    "messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the model again to answer the original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=os.environ[\"GPT_4_MODEL_NAME\"],  # need a model version 0613 or higher\n",
    "    messages=messages,\n",
    "    seed=0,\n",
    "    tools=tools,\n",
    ")  # get a new response using the result of the function call\n",
    "\n",
    "\n",
    "message = response.choices[0].message.content\n",
    "tool_calls = response.choices[0].message.tool_calls\n",
    "\n",
    "print(f\"New tool calls: {tool_calls}\")\n",
    "print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to calling the function, we were able to produce the correct result!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model will only reach for a function if it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_problem = \"How many wives did king Henry VIII of England have?\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=os.environ[\"GPT_4_MODEL_NAME\"],  # need a model past 0613\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": history_problem},\n",
    "    ],\n",
    "    seed=0,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "response_message = response.choices[0].message\n",
    "message = response_message.content\n",
    "tool_calls = response_message.tool_calls\n",
    "\n",
    "print(f\"Message: {message}\")\n",
    "print(f\"Tool calls: {tool_calls}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External APIs\n",
    "\n",
    "One way of gathering live information is through the use of an external API.\n",
    "\n",
    "They can enable us to retrieve a wide variety of information (although some will come at a cost for commercial usage). For example:\n",
    "- The weather conditions for a particular location.\n",
    "- The topic headline from a particular news source.\n",
    "- The recent prices of a stock .\n",
    "\n",
    "\n",
    "### Weather\n",
    "\n",
    "To get information about local weather we can use the API from https://www.weatherapi.com, which is availble on a [free plan](https://www.weatherapi.com/pricing.aspx). An API key has been provided for you in the Codespace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function below to obtain weather information for a particular location. You can take a look at the [WeatherAPI docs](https://www.weatherapi.com/docs/) for more examples of how to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"Get the current weather conditions in a given location\n",
    "\n",
    "    :param location: The city (and state), e.g. \"San Francisco, CA\"\n",
    "    :return: The weather conditions\n",
    "    \"\"\"\n",
    "    return requests.get(\n",
    "        f\"https://api.weatherapi.com/v1/current.json?key={os.environ['WEATHER_API_KEY']}&q={location}\"\n",
    "    ).json()\n",
    "\n",
    "\n",
    "get_current_weather(\"London\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information in the JSON that the function returns, more than is needed for the average person. However, a LLM would be able to summarize this information into a readable paragrpah.\n",
    "\n",
    "In the cell below we have written a system prompt and user prompt, which we pass to the function `function_calling_agent`, along with the `get_current_weather` function. \n",
    "\n",
    "The `function_calling_agent` is a helpful function we wrote to automate the reasoning process used above:\n",
    "- First, any functions we want to give the model access to are transformed into the JSON format it expects.\n",
    "- We then extract a response from a model, based on the input prompts. \n",
    "- The process may finish there, but if the model deems if necessary, it may ask for a function to be called. In this case:\n",
    "    - A function is called using the parameters returned by the model,\n",
    "    - The function response is then given to the model.\n",
    "    - Finally, the output to this new infromation is returned.\n",
    "\n",
    "We have added some print statements to help you understand what is going on \"under the hood\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a weather bot that is able to give back a summary of the weather conditions in the given location. \n",
    "\n",
    "You do not return many measurements about the conditions.\n",
    "\n",
    "Instead, you give an overall idea as to what conditions are like in one or two sentences.\"\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"What is the weather in London?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = function_calling_agent(client, system_prompt, user_prompt, get_current_weather)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1a: News\n",
    "\n",
    "Your task is to build an agent that can summarize the top headlines of the day from a given news source.\n",
    "\n",
    "To give the model access to the latest news stories, you can use the https://newsapi.org/ API, which is available on a [free plan](https://newsapi.org/pricing). An API key is already provided for you in the Codespace.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part i: The day's top headlines\n",
    "\n",
    "Write a Python function that returns JSON information about the top headlines of the day (from a given source) in the Python dictionary format. You can use the examples given in the [NewsAPI docs](https://newsapi.org/docs/endpoints/top-headlines) to help you answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_headlines(source: str) -> dict:\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "\n",
    "get_top_headlines(\"bbc-news\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ii: Prompt engineering for a news summary\n",
    "\n",
    "Write a system prompt and user prompt that will lead the agent to summarize the day's top headlines. \n",
    "\n",
    "Rather than simply returning a list of the top stories, the agent should be able to condense the information into a few sentences. The agent should also be capable of only writing about the news category the user is interested in, e.g. politics or sport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = function_calling_agent(client, system_prompt, user_prompt, get_top_headlines)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b: Stocks\n",
    "\n",
    "In this exercise you will build a stock analysis agent to help a user to make investment decisions.\n",
    "\n",
    "The agent will be able examine the recent history of a stock, along with recent press articles about it, to determine whether or not the stock is a good investment.\n",
    "\n",
    "<mark>Warning! This agent is only intended to be used for educational purpose and we do not expect its suggestions to be of much worth.</mark> \n",
    "\n",
    "<mark> Xebia will not be held liable for any financial losses incurred from its usage!</mark> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the recent history of a stock we will use [yfinance](https://github.com/ranaroussi/yfinance). yfinance offers a threaded and Pythonic way to download market data from [Yahoo!\u24c7 finance](https://finance.yahoo.com). Note that it is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes.\n",
    "\n",
    "Given a stock ticker symbol, we can retrive recent information about that stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = yf.Ticker(\"IBM\")\n",
    "df = stock.history(period=f\"30d\")\n",
    "df = df[[\"Close\", \"Volume\"]]\n",
    "df.index = [str(x).split()[0] for x in list(df.index)]\n",
    "df.index.rename(\"Date\", inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part i: Recent stock history\n",
    "\n",
    "Write a Python function that returns JSON information about the recent history of a particular stock in the Python dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_stock_prices(company_stock_ticker_symbol: str, days: int = 30) -> dict:\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "    return df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_prices(\"IBM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use NewsAPI again to retrieve the recent headlines about a particular stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.Timestamp.today()\n",
    "start = pd.Timestamp.today()\n",
    "end = today - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"IBM stock news\"\n",
    "\n",
    "request = requests.get(\n",
    "    f\"https://newsapi.org/v2/everything?q={topic}&from={start}&to={end}&sortBy=popularity&apiKey={os.environ['NEWS_API_KEY']}\"\n",
    ").json()\n",
    "articles = pd.DataFrame(request[\"articles\"])\n",
    "titles = articles[\"title\"]\n",
    "titles\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ii: Any recent news?\n",
    "\n",
    "Write a Python function that returns JSON information about the recent headlines of a particulaer stock in the Python dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_stories(topic: str) -> dict:\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "    return titles.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_news_stories(\"Apple\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part iii: Prompt engineering for a stock\n",
    "\n",
    "Write a system prompt and user prompt that will lead the agent to determine whether or not a stock is a good investment. \n",
    "\n",
    "As you may have already experienced with ChatGPT, the model will want to caveat its opinions.\n",
    "For the purpose of this assignment, see if you can get it to provide an opinion without any reservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = function_calling_agent(client, system_prompt, user_prompt, get_news_stories, get_stock_prices)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part iv: Prompt engineering for multiple stocks.\n",
    "\n",
    "Update your system/user prompt so that it will lead the agent to make an investment decision for more than one stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}