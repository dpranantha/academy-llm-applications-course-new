{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Tool Calling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large Language Models (LLMs) are incredibly powerful. However, as we've seen, they can struggle with simple mathematical problems and are limited by the information contained in their training data.\n",
    "\n",
    "That said, we can describe tools, such as Python functions, to the LLM that may be useful for solving tasks. While the models themselves won't call the functions directly, they can determine when a function is needed and return the appropriate arguments for use.\n",
    "\n",
    "In this notebook, you'll learn how describing functions to a model can transform it into an agent capable of reasoning with itself to answer a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload imports\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.llm import instantiate_langchain_model\n",
    "from llm_in_production.agent_utils import (\n",
    "    # function_to_json,\n",
    "    tool_calling_agent,\n",
    ")\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "import dotenv\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import tiktoken\n",
    "\n",
    "# This reads the .env file in your project and transforms its content into env variables.\n",
    "# This way you don't have to hard code your secrets.\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Here we create the client. \n",
    "# Make sure you select the LLM provider that corresponds to the one you are using in this course!\n",
    "client = instantiate_langchain_model(\n",
    "    # llm_provider=\"azure\",\n",
    "    llm_provider=\"gcp\",\n",
    ")\n",
    "client.model_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM limiations\n",
    "\n",
    "Let's start by demonstrating one of the limitations of LLMs.\n",
    "\n",
    "The prompt below is asking the model to answer a simple highschool mathematics problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_problem = \"what is the circumference of a circle with radius 5.31cm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a calculator bot that is used to answer mathematics problems\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": math_problem},\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "response = client.invoke(\n",
    "    input=messages,\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "message = response.content\n",
    "\n",
    "print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the answer it produces is incorrect.\n",
    "\n",
    "$2*\\pi*5.31 = 33.3637139811...$\n",
    "\n",
    "For this problem, a simple Python function would be more useful!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circumference_calculator(radius: float, something: float = 4.4) -> float:\n",
    "    \"\"\"Calculates the circumference of a circle given the radius\n",
    "\n",
    "    :param radius: The radius of the circle\n",
    "    :return: The circumference of the circle\n",
    "    \"\"\"\n",
    "    return 2 * np.pi * radius\n",
    "\n",
    "\n",
    "circumference_calculator(5.31)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if the LLM knew about this Python function?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tool Calling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now possible to inform LLMs about external tools.\n",
    "\n",
    "The model can then determine on its own when it's appropriate to use one of these tools and, if needed, return the appropriate arguments."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To describe a function to the model it must be in a particular JSON format. LangChain have created a helpful function to transform Python functions into this format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circumference_calculator_json = convert_to_openai_function(circumference_calculator)\n",
    "circumference_calculator_json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give the model access to this tool and repeat the prompt we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [{\"type\": \"function\", \"function\": circumference_calculator_json}]\n",
    "\n",
    "response = client.invoke(\n",
    "    input=messages,\n",
    "    seed=0,\n",
    "    tools=tools,\n",
    "    tool_choice = \"auto\"  # (default setting) the model will pick between generating a message or calling a function automatically \n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has no message to return:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = response.content\n",
    "print(f\"Message: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, it has called for a tool to be used, with arguments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_calls = response.tool_calls\n",
    "print(f\"Tool calls: {tool_calls}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do what the model suggests and call the Python function `circumference_calculator`, with the argument `radius=5.31`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_name = tool_calls[0]['name']\n",
    "tool_args = tool_calls[0]['args']\n",
    "tool_response = json.dumps(eval(tool_name)(**tool_args))\n",
    "\n",
    "print(f\"Function response {tool_response}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number is correct. Let's pass information back to the model.\n",
    "\n",
    "First we need to update the message history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response)\n",
    "\n",
    "messages.append(\n",
    "    {\n",
    "        \"role\": \"tool\",\n",
    "        \"name\": tool_name,\n",
    "        \"content\": tool_response,\n",
    "        \"tool_call_id\": tool_calls[0]['id'],\n",
    "    }\n",
    ")  # extend conversation with function response\n",
    "\n",
    "if len(messages) > 4:\n",
    "    raise Exception(\n",
    "        \"Too many messages have been added! Restart and rerun the notebook.\"\n",
    "    )\n",
    "\n",
    "messages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the model again to answer the original query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.invoke(\n",
    "    input=messages,\n",
    "    seed=0,\n",
    "    tools=tools,\n",
    ")  # get a new response using the result of the function call\n",
    "\n",
    "\n",
    "message = response.content\n",
    "tool_calls = response.tool_calls\n",
    "\n",
    "print(f\"New tool calls: {len(tool_calls)}\")\n",
    "print(message)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to tool calling, we were able to produce the correct result!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model will only reach for a function if it is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_problem = \"How many wives did king Henry VIII of England have?\"\n",
    "\n",
    "response = client.invoke(\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": history_problem},\n",
    "    ],\n",
    "    seed=0,\n",
    "    tools=tools,\n",
    ")\n",
    "\n",
    "message = response.content\n",
    "tool_calls = response.tool_calls\n",
    "\n",
    "print(f\"Message: {message}\")\n",
    "print(f\"Tool calls: {tool_calls}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External APIs\n",
    "\n",
    "One way of gathering live information is through the use of an external API.\n",
    "\n",
    "They can enable us to retrieve a wide variety of information (although some will come at a cost for commercial usage). For example:\n",
    "- The weather conditions for a particular location.\n",
    "- The topic headline from a particular news source.\n",
    "- The recent prices of a stock .\n",
    "\n",
    "\n",
    "### Weather\n",
    "\n",
    "To get information about local weather we can use the API from https://www.weatherapi.com, which is availble on a [free plan](https://www.weatherapi.com/pricing.aspx). An API key has been provided for you in the Codespace."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function below to obtain weather information for a particular location. You can take a look at the [WeatherAPI docs](https://www.weatherapi.com/docs/) for more examples of how to use the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"Get the current weather conditions in a given location\n",
    "\n",
    "    :param location: The city (and state), e.g. \"San Francisco, CA\"\n",
    "    :return: The weather conditions\n",
    "    \"\"\"\n",
    "    return requests.get(\n",
    "        f\"https://api.weatherapi.com/v1/current.json?key={os.environ['WEATHER_API_KEY']}&q={location}\"\n",
    "    ).json()\n",
    "\n",
    "\n",
    "get_current_weather(\"London\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot of information in the JSON that the function returns, more than is needed for the average person. However, an LLM would be able to summarize this information into a readable paragraph.\n",
    "\n",
    "In the cell below, we have written a system prompt and user prompt, which we pass to the `tool_calling_agent` function, along with the `get_current_weather` function.\n",
    "\n",
    "The `tool_calling_agent` is a helpful function we wrote to automate the reasoning process used above:\n",
    "\n",
    "- First, any functions or tools we want to give the model access to are transformed into the JSON format it expects.\n",
    "- We then extract a response from the model based on the input prompts.\n",
    "- The process may finish there, but if the model deems it necessary, it may ask for a tool to be called. In this case:\n",
    "    - The tool is called using the parameters returned by the model.\n",
    "    - The tool\u2019s response is then given to the model.\n",
    "    - Finally, the output based on this new information is returned.\n",
    "    \n",
    "We\u2019ve added some logging to help you understand what\u2019s going on \"under the hood.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a weather bot that is able to give back a summary of the weather conditions in the given location. \n",
    "\n",
    "You do not return many measurements about the conditions.\n",
    "\n",
    "Instead, you give an overall idea as to what conditions are like in one or two sentences.\"\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"What is the weather in London?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tool_calling_agent(client, system_prompt, user_prompt, get_current_weather)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1a: News\n",
    "\n",
    "Your task is to build an agent that can summarize the top headlines of the day from a given news source.\n",
    "\n",
    "To give the model access to the latest news stories, you can use the https://newsapi.org/ API, which is available on a [free plan](https://newsapi.org/pricing). An API key is already provided for you in the Codespace.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part i: The day's top headlines\n",
    "\n",
    "Write a Python function that returns JSON information about the top headlines of the day (from a given source) in the Python dictionary format. You can use the examples given in the [NewsAPI docs](https://newsapi.org/docs/endpoints/top-headlines) to help you answer this question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_headlines(source: str) -> dict:\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "\n",
    "get_top_headlines(\"bbc-news\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ii: Prompt engineering for a news summary\n",
    "\n",
    "Write a system prompt and user prompt that will lead the agent to summarize the day's top headlines. \n",
    "\n",
    "Rather than simply returning a list of the top stories, the agent should be able to condense the information into a few sentences. The agent should also be capable of only writing about the news category the user is interested in, e.g. politics or sport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tool_calling_agent(client, system_prompt, user_prompt, get_top_headlines)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1b: Stocks\n",
    "\n",
    "In this exercise you will build a stock analysis agent to help a user to make investment decisions.\n",
    "\n",
    "The agent will be able examine the recent history of a stock, along with recent press articles about it, to determine whether or not the stock is a good investment.\n",
    "\n",
    "<mark>Warning! This agent is only intended to be used for educational purpose and we do not expect its suggestions to be of much worth.</mark> \n",
    "\n",
    "<mark> Xebia will not be held liable for any financial losses incurred from its usage!</mark> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the recent history of a stock we will use [yfinance](https://github.com/ranaroussi/yfinance). yfinance offers a threaded and Pythonic way to download market data from [Yahoo!\u24c7 finance](https://finance.yahoo.com). Note that it is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes.\n",
    "\n",
    "Given a stock ticker symbol, we can retrive recent information about that stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = yf.Ticker(\"IBM\")\n",
    "df = stock.history(period=\"1mo\")\n",
    "df = df[[\"Close\", \"Volume\"]]\n",
    "print(df.index.max() - df.index.min())\n",
    "df.index = [str(x).split()[0] for x in list(df.index)]\n",
    "df.index.rename(\"Date\", inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part i: Recent stock history\n",
    "\n",
    "Write a Python function that returns JSON information about the recent history of a particular stock in the Python dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_prices(company_stock_ticker_symbol: str, period: str = \"1mo\") -> dict:\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "    return df.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_stock_prices(\"IBM\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use NewsAPI again to retrieve the recent headlines about a particular stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.Timestamp.today()\n",
    "start = pd.Timestamp.today()\n",
    "end = today - pd.Timedelta(days=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"IBM stock news\"\n",
    "\n",
    "request = requests.get(\n",
    "    f\"https://newsapi.org/v2/everything?q={topic}&from={start}&to={end}&sortBy=popularity&apiKey={os.environ['NEWS_API_KEY']}\"\n",
    ").json()\n",
    "articles = pd.DataFrame(request[\"articles\"])\n",
    "titles = articles[\"title\"]\n",
    "titles\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part ii: Any recent news?\n",
    "\n",
    "Write a Python function that returns JSON information about the recent headlines of a particulaer stock in the Python dictionary format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_stories(topic: str) -> dict:\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "    return titles.to_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_news_stories(\"Apple\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part iii: Prompt engineering for a stock\n",
    "\n",
    "Write a system prompt and user prompt that will lead the agent to determine whether or not a stock is a good investment. \n",
    "\n",
    "As you may have already experienced with ChatGPT, the model will want to caveat its opinions.\n",
    "For the purpose of this assignment, see if you can get it to provide an opinion without any reservations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tool_calling_agent(client, system_prompt, user_prompt, get_news_stories, get_stock_prices)\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part iv: Prompt engineering for multiple stocks.\n",
    "\n",
    "Update your system/user prompt so that it will lead the agent to make an investment decision for more than one stock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}