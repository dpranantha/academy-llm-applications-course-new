{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "634845907b69df43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3: Chunking\n",
    "In the previous exercise, we searched through the PyData talks based on their title, abstracts, and descriptions.\n",
    "However, as we noticed in the story example, embedding large pieces of text is not always the best approach.\n",
    "The main reason for this is that an embedding can only capture so much information.\n",
    "So, if a piece of text becomes longer, the embedding will become less accurate.\n",
    "Therefore, it is often better to split the text into smaller chunks and embed each chunk separately.\n",
    "\n",
    "In this exercise we will demonstrate how to perform chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a22746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from llm_in_production.huggingface_utils import get_device\n",
    "from llm_in_production.openai_utils import get_number_of_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ea6644f",
   "metadata": {},
   "source": [
    "We shall be working with the PyData talks dataset, so let's start by loading that in again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1974931",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pydata.json\", \"r\") as f:\n",
    "    talks = json.load(f)[\"talks\"]\n",
    "    titles = [talk[\"title\"] for talk in talks]\n",
    "    abstracts = [talk[\"abstract\"] for talk in talks]\n",
    "    descriptions = [talk[\"description\"] for talk in talks]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30101bed",
   "metadata": {},
   "source": [
    "We will again use the [MiniLM](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model from [HuggingFace](https://huggingface.co/) to embed the text. To make it easier to use, we have wrapped the model in a class called [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) from LangChain. Running the cell below will download the model from the HuggingFace model hub and load it into memory. This can take a while the first time you run it. However, the model will be cached on your computer, so it will be much faster the next time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1676cd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function check if the accelerator is available like a GPU and if so, it will use it.\n",
    "device = get_device()\n",
    "# Here we create the embedding function that will be used to embed the sentences.\n",
    "embedding_func = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": get_device()})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc5ae4e",
   "metadata": {},
   "source": [
    "## Exercise 3a: Introduction to text splitting and chunking\n",
    "\n",
    "In the cell below, we have an example of how to split a text into chunks.\n",
    "\n",
    "We use the [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html) from LangChain, which attempts to split the text using different separator characters, until it finds chunks of a small enough size.\n",
    "\n",
    "The main reason for using this recursive approach is that it allows us to split the text into chunks of roughly the same size.\n",
    "\n",
    "Please run the cell below and do the following:\n",
    "- Change the `chunk_size` and `chunk_overlap` and see how the results change.\n",
    "- Change the `keep_separator` and see how the results change.\n",
    "- Add or remove separators and see how the results change.\n",
    "- Currently, we are using the `get_number_of_tokens` function to determine how to measure the lengths of the chunks. Change the `length_function` to `len` and see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abdb5fc7f4e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"],\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 25,\n",
    "    length_function = get_number_of_tokens,\n",
    "    keep_separator=True,\n",
    "    strip_whitespace=True,\n",
    ")\n",
    "\n",
    "talk_idx = 5\n",
    "title = titles[talk_idx]\n",
    "abstract = abstracts[talk_idx]\n",
    "description = descriptions[talk_idx]\n",
    "all_text = f\"\"\"\n",
    "Title: {title}\n",
    "Abstract: \n",
    "{abstract}\n",
    "Description:\n",
    "{description}\n",
    "\"\"\"\n",
    "\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i} (n_char={len(chunk)} n_tokens={get_number_of_tokens(chunk)}):\")\n",
    "    print(chunk)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b24db5504578e94",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 3b: Incorporating chunking to get better search results.\n",
    "We have now seen how to split a text into chunks.\n",
    "Let's see if we can use it to get better search results on the PyData talks dataset.\n",
    "\n",
    "We will do the following:\n",
    "1. Create a `RecursiveCharacterTextSplitter` that splits the text into chunks of roughly 100 tokens, with an overlap of 10-30 tokens.\n",
    "2. Then, for each talk, we do the following:\n",
    "    1. Combine the title, abstract, and description into a single string.\n",
    "    1. Split the text into chunks using the `RecursiveCharacterTextSplitter`.\n",
    "    1. Store each chunk in the vector database and make all the chunks of the same talk have the same metadata:\n",
    "        - The `title` of the talk.\n",
    "        - The `talk_idx`, which is the index of the talk in the `talks` list.\n",
    "3. We can then build the vector database around the chunks and their metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c842fc2f7f84ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # YOUR CODE HERE START: define the separators, chunk_size and chunk_overlap here.\n",
    "     # YOUR CODE HERE END\n",
    "    length_function = get_number_of_tokens,\n",
    "    keep_separator=True,\n",
    "    strip_whitespace=True,\n",
    ")\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for talk_idx in range(len(talks)):\n",
    "    title = titles[talk_idx]\n",
    "    abstract = abstracts[talk_idx]\n",
    "    description = descriptions[talk_idx]\n",
    "    \n",
    "    metadata = {\"title\": title, \"talk_idx\": talk_idx}\n",
    "    all_text = \"\"\n",
    "    # YOUR CODE HERE START: Combine the title, abstract, and description into a single string.\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    # YOUR CODE HERE START: Split the text into chunks using the RecursiveCharacterTextSplitter.\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        texts.append(chunk)\n",
    "        metadatas.append(metadata)\n",
    "\n",
    "\n",
    "assert len(texts) == len(metadatas), f\"{len(texts)} != {len(metadatas)}\"\n",
    "    \n",
    "# YOUR CODE HERE START: Create a vector database around the texts and their metadata. (Hint: use the FAISS.from_texts method) \n",
    "# YOUR CODE HERE END"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dd295767d080c35",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try out some different queries in the cell below. Do you get better results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a7ef2d4dc19f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'which talks are about LLM?'\n",
    "# query = 'which talks are about data engineering?'\n",
    "# query = 'which talks combine bayesian and llm?'\n",
    "k = 3\n",
    "documents = vector_database.similarity_search(query, k=k)\n",
    "\n",
    "for document in documents:\n",
    "    title = document.metadata['title']\n",
    "    page_content = document.page_content\n",
    "    \n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Page content: {page_content}\")\n",
    "    print(\"#\" * 80 +\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}