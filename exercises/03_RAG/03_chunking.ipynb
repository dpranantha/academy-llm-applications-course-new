{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "634845907b69df43",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3: Chunking\n",
    "In the previous exercise, we searched through the PyData talks based on their title, abstracts, and descriptions.\n",
    "However, as we noticed in the story example, embedding large pieces of text is not always the best approach.\n",
    "The main reason for this is that an embedding can only capture so much information.\n",
    "So, if a piece of text becomes longer, the embedding will become less accurate.\n",
    "Therefore, it is often better to split the text into smaller chunks and embed each chunk separately.\n",
    "\n",
    "In this exercise we will demonstrate how to perform chunking."
   ]
  },
  {
   "cell_type": "code",
   "id": "94a22746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:54:29.658082Z",
     "start_time": "2025-03-06T12:54:29.653482Z"
    }
   },
   "source": [
    "import json\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from llm_in_production.huggingface_utils import get_device\n",
    "from llm_in_production.llm import instantiate_langchain_model\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "client = instantiate_langchain_model(\n",
    "    # llm_provider=\"azure\",\n",
    "    llm_provider=\"gcp\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ea6644f",
   "metadata": {},
   "source": [
    "We shall be working with the PyData talks dataset, so let's start by loading that in again."
   ]
  },
  {
   "cell_type": "code",
   "id": "d1974931",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:54:39.145807Z",
     "start_time": "2025-03-06T12:54:39.142107Z"
    }
   },
   "source": [
    "with open(\"pydata.json\", \"r\") as f:\n",
    "    talks = json.load(f)[\"talks\"]\n",
    "    titles = [talk[\"title\"] for talk in talks]\n",
    "    abstracts = [talk[\"abstract\"] for talk in talks]\n",
    "    descriptions = [talk[\"description\"] for talk in talks]"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30101bed",
   "metadata": {},
   "source": [
    "We will again use the [MiniLM](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model from [HuggingFace](https://huggingface.co/) to embed the text. To make it easier to use, we have wrapped the model in a class called [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) from LangChain. Running the cell below will download the model from the HuggingFace model hub and load it into memory. This can take a while the first time you run it. However, the model will be cached on your computer, so it will be much faster the next time you run it."
   ]
  },
  {
   "cell_type": "code",
   "id": "1676cd20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:54:44.245724Z",
     "start_time": "2025-03-06T12:54:42.152168Z"
    }
   },
   "source": [
    "# This function check if the accelerator is available like a GPU and if so, it will use it.\n",
    "device = get_device()\n",
    "# Here we create the embedding function that will be used to embed the sentences.\n",
    "embedding_func = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": get_device()})"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cc5ae4e",
   "metadata": {},
   "source": [
    "## Exercise 3a: Introduction to text splitting and chunking\n",
    "\n",
    "In the cell below, we have an example of how to split a text into chunks.\n",
    "\n",
    "We use the [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html) from LangChain, which attempts to split the text using different separator characters, until it finds chunks of a small enough size.\n",
    "\n",
    "The main reason for using this recursive approach is that it allows us to split the text into chunks of roughly the same size.\n",
    "\n",
    "Please run the cell below and do the following:\n",
    "- Change the `chunk_size` and `chunk_overlap` and see how the results change.\n",
    "- Change the `keep_separator` and see how the results change.\n",
    "- Add or remove separators and see how the results change.\n",
    "- Currently, we are using the `client`'s built-in `get_num_tokens` method to determine how to measure the lengths of the chunks. Change the `length_function` to `len` and see how the results change."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3abdb5fc7f4e3c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:55:07.210390Z",
     "start_time": "2025-03-06T12:55:03.913627Z"
    }
   },
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"],\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 25,\n",
    "    length_function = client.get_num_tokens,\n",
    "    keep_separator=True,\n",
    "    strip_whitespace=True,\n",
    ")\n",
    "\n",
    "talk_idx = 5\n",
    "title = titles[talk_idx]\n",
    "abstract = abstracts[talk_idx]\n",
    "description = descriptions[talk_idx]\n",
    "all_text = f\"\"\"\n",
    "Title: {title}\n",
    "Abstract: \n",
    "{abstract}\n",
    "Description:\n",
    "{description}\n",
    "\"\"\"\n",
    "\n",
    "chunks = text_splitter.split_text(all_text)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i} (n_char={len(chunk)} n_tokens={client.get_num_tokens(chunk)}):\")\n",
    "    print(chunk)\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 (n_char=508 n_tokens=99):\n",
      "Title: Power Users, Long Tail Users, and Everything In Between: Choosing Meaningful Metrics and KPIs for Product Strategy\n",
      "Abstract: \n",
      "Data scientists in industry often have to wear many hats. They must navigate statistical validity, business acumen and strategic thinking, while also representing the end user. In this talk, we will talk about the pillars that make a metric the right one for a job, and how to choose appropriate Key Performance Indicators (KPIs) to drive product success and strategic gains.\n",
      "\n",
      "Chunk 1 (n_char=508 n_tokens=95):\n",
      "Description:\n",
      "Our presentation will traverse the relationship of data science skills in product strategy - embracing the multifaceted role of the data scientist and navigating the journey from user segmentation to making data-driven decisions.\r\n",
      "\r\n",
      "1. The Data Scientist's Hat Trick: We initiate by emphasising the assorted roles that a data scientist plays in today's business landscape - from being a statistician ensuring the accuracy and validity of data to a strategist driving business decisions. [5 mins]\n",
      "\n",
      "Chunk 2 (n_char=318 n_tokens=62):\n",
      "2. Choosing Significant Metrics: Next, we'll delve into the nuances of selecting the right metric for the job. Specifically, weâ€™ll talk about the different pillars of metrics setting, for common data science responsibilities such as randomised controlled trials, offline evaluation, opportunity analysis etc.  [7 mins]\n",
      "\n",
      "Chunk 3 (n_char=183 n_tokens=43):\n",
      "3. Setting The Right KPIs: Once metrics are defined, we'll venture into setting the correct KPIs - the small set of top line numbers that say if our venture is doing the job. [7 mins]\n",
      "\n",
      "Chunk 4 (n_char=416 n_tokens=88):\n",
      "4. Data-Driven Decision Making: Lastly, we'll elucidate how to leverage the data you've gathered to make informed, strategic decisions. This necessitates interpreting your metrics and KPIs, spotting trends, and making necessary adjustments to stay on course. [7 mins]\r\n",
      "\r\n",
      "Incorporating real-world case studies, we'll demonstrate how these concepts intertwine to contribute to product success. \r\n",
      "\r\n",
      "Learning Objectives:\n",
      "\n",
      "Chunk 5 (n_char=485 n_tokens=88):\n",
      "Learning Objectives:\r\n",
      "* Appreciate the multifaceted role of a data scientist in driving product strategies.\r\n",
      "* Learn to set realistic and challenging KPIs that align with your company's overarching objectives.\r\n",
      "* Gain insights into leveraging data for informed decision-making and product strategy adjustments.\r\n",
      "\r\n",
      "Who Should Attend:\r\n",
      "This talk is aimed for data professionals, however anyone involved in shaping product strategy and making data-driven decisions could find this useful.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6b24db5504578e94",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 3b: Incorporating chunking to get better search results.\n",
    "We have now seen how to split a text into chunks.\n",
    "Let's see if we can use it to get better search results on the PyData talks dataset.\n",
    "\n",
    "We will do the following:\n",
    "1. Create a `RecursiveCharacterTextSplitter` that splits the text into chunks of roughly 100 tokens, with an overlap of 10-30 tokens.\n",
    "2. Then, for each talk, we do the following:\n",
    "    1. Combine the title, abstract, and description into a single string.\n",
    "    1. Split the text into chunks using the `RecursiveCharacterTextSplitter`.\n",
    "    1. Store each chunk in the vector database and make all the chunks of the same talk have the same metadata:\n",
    "        - The `title` of the talk.\n",
    "        - The `talk_idx`, which is the index of the talk in the `talks` list.\n",
    "3. We can then build the vector database around the chunks and their metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63c842fc2f7f84ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T13:35:25.422272Z",
     "start_time": "2025-03-06T13:16:11.073105Z"
    }
   },
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # YOUR CODE HERE START: define the separators, chunk_size and chunk_overlap here.\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \"?\", \"!\", \" \", \"\"],\n",
    "    chunk_size= 100,\n",
    "    chunk_overlap=10,\n",
    "     # YOUR CODE HERE END\n",
    "    length_function = client.get_num_tokens,\n",
    "    keep_separator=True,\n",
    "    strip_whitespace=True,\n",
    ")\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for talk_idx in range(len(talks)):\n",
    "    title = titles[talk_idx]\n",
    "    abstract = abstracts[talk_idx]\n",
    "    description = descriptions[talk_idx]\n",
    "    \n",
    "    metadata = {\"title\": title, \"talk_idx\": talk_idx}\n",
    "    all_text = f\"\"\"\n",
    "        Title: {title} Abstract: {abstracts} Description: {description}\n",
    "    \"\"\".strip()\n",
    "    # YOUR CODE HERE START: Combine the title, abstract, and description into a single string.\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    # YOUR CODE HERE START: Split the text into chunks using the RecursiveCharacterTextSplitter.\n",
    "    chunks = text_splitter.split_text(all_text)\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        texts.append(chunk)\n",
    "        metadatas.append(metadata)\n",
    "\n",
    "\n",
    "assert len(texts) == len(metadatas), f\"{len(texts)} != {len(metadatas)}\"\n",
    "    \n",
    "# YOUR CODE HERE START: Create a vector database around the texts and their metadata. (Hint: use the FAISS.from_texts method)\n",
    "vector_database = FAISS.from_texts(texts, metadatas=metadatas, embedding=embedding_func)\n",
    "# YOUR CODE HERE END"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8dd295767d080c35",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Try out some different queries in the cell below. Do you get better results?"
   ]
  },
  {
   "cell_type": "code",
   "id": "bf4a7ef2d4dc19f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T13:35:35.735304Z",
     "start_time": "2025-03-06T13:35:34.283116Z"
    }
   },
   "source": [
    "query = 'which talks are about LLM?'\n",
    "# query = 'which talks are about data engineering?'\n",
    "# query = 'which talks combine bayesian and llm?'\n",
    "k = 3\n",
    "documents = vector_database.similarity_search(query, k=k)\n",
    "\n",
    "for document in documents:\n",
    "    title = document.metadata['title']\n",
    "    page_content = document.page_content\n",
    "    \n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Page content: {page_content}\")\n",
    "    print(\"#\" * 80 +\"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: What the PDEP? An overview of some upcoming pandas changes\n",
      "Page content: .\\r\\n\\r\\nThis talk is for people who want to learn how to build their first LLM-based agent. Familiarity with Python, PyDantic, and LMMs is nice during this presentation but not essential. As long as you love overengineered solutions to a basic to-do list, you will like this presentation.', 'The continued success of large language models (LLMs) hinges upon accurate, diverse, and well-labeled data\n",
      "################################################################################\n",
      "\n",
      "Title: Turning your Data/AI algorithms into full web applications in no time with Taipy\n",
      "Page content: .\\r\\n\\r\\nThis talk is for people who want to learn how to build their first LLM-based agent. Familiarity with Python, PyDantic, and LMMs is nice during this presentation but not essential. As long as you love overengineered solutions to a basic to-do list, you will like this presentation.', 'The continued success of large language models (LLMs) hinges upon accurate, diverse, and well-labeled data\n",
      "################################################################################\n",
      "\n",
      "Title: Tables as Code: The Journey from Ad-hoc Scripts to Maintainable ETL Workflows at Booking.com\n",
      "Page content: .\\r\\n\\r\\nThis talk is for people who want to learn how to build their first LLM-based agent. Familiarity with Python, PyDantic, and LMMs is nice during this presentation but not essential. As long as you love overengineered solutions to a basic to-do list, you will like this presentation.', 'The continued success of large language models (LLMs) hinges upon accurate, diverse, and well-labeled data\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "82d7da1e",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
