{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b644ea81fd0ae40f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 2: Introduction to vector database\n",
    "In the previous exercise, we explored the clustering capabilities of embedding models. \n",
    "\n",
    "This notebook will explore how to use these properties to find semantically similar items.\n",
    "\n",
    "We will do this first by sorting the items by their similarity score; the higher the score, the more similar the items are.\n",
    "\n",
    "However, this approach does not scale well to large datasets due to the $O(n log (n))$ complexity of sorting. Therefore, we will also explore how to use vector databases to find similar items in a more efficient way.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "e4d365831745c4ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:12:24.873430Z",
     "start_time": "2025-03-06T11:12:20.045527Z"
    }
   },
   "source": [
    "import json\n",
    "from operator import contains\n",
    "\n",
    "import numpy as np\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from llm_in_production.huggingface_utils import get_device\n",
    "from llm_in_production.numpy_utils import cosine_similarity\n",
    "from langchain.vectorstores import FAISS\n",
    "import dotenv\n",
    "import pandas as pd\n",
    "\n",
    "dotenv.load_dotenv()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d22ec368bb32cd9b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will again use the [MiniLM](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) model from [HuggingFace](https://huggingface.co/) to embed the text. To make it easier to use, we have wrapped the model in a class called [HuggingFaceEmbeddings](https://api.python.langchain.com/en/latest/embeddings/langchain.embeddings.huggingface.HuggingFaceEmbeddings.html) from LangChain. Running the cell below will download the model from the HuggingFace model hub and load it into memory. This can take a while the first time you run it. However, the model will be cached on your computer, so it will be much faster the next time you run it."
   ]
  },
  {
   "cell_type": "code",
   "id": "8f950abf9d37f561",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:12:51.936337Z",
     "start_time": "2025-03-06T11:12:49.368542Z"
    }
   },
   "source": [
    "# This function check if the accelerator is available like a GPU and if so, it will use it.\n",
    "device = get_device()\n",
    "# Here we create the embedding function that will be used to embed the sentences.\n",
    "embedding_func = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"device\": get_device()})"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb86914c057836c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2a: Finding similar items by sorting\n",
    "Here, we will explore how to find similar items using cosine similarity.\n",
    "We do this by taking the following steps:\n",
    "1. We embed all the items we want to search through. \n",
    "2. We embed the query. \n",
    "3. We compute the cosine similarity between the query and all the items. This will give us a score for each item. The higher the score, the more similar the item is to the query.\n",
    "4. We sort the items by their score. The higher the score, the more similar the item is to the query.\n",
    "\n",
    "The code below does everything except for the sorting. Your task is to sort the items by their score using the [df.sort_values](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.sort_values.html) method from pandas.\n",
    "\n",
    "\n",
    "After finishing the code, experiment with different queries and see how the results change. Also, try to change the sentences to see how that affects the results."
   ]
  },
  {
   "cell_type": "code",
   "id": "941a2b7d886b4943",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:13:28.056507Z",
     "start_time": "2025-03-06T11:13:24.383242Z"
    }
   },
   "source": [
    "sentences = [\n",
    "    \"Merel said something about cats.\",\n",
    "    \"Merel said dogs are awesome.\",\n",
    "    \"Samantha's buddies said to meet them in the bar\",\n",
    "    \"The cat is walking in the bedroom\",\n",
    "    \"The kittens are in the bedroom\",\n",
    "    \"A dogs were running across the kitchen\",\n",
    "    \"The puppies were running around in the kitchen\",\n",
    "]\n",
    "# Here we embed all the sentences at once using the embed_documents method\n",
    "sentence_embeddings = embedding_func.embed_documents(sentences)\n",
    "# We convert the embeddings to a numpy array/matrix of shape (n_samples, n_features).\n",
    "sentence_embeddings = np.array(sentence_embeddings)\n",
    "\n",
    "\n",
    "query = \"What did she say?\"\n",
    "# query = \"what are the puppies doing?\" # This is an alternative query that you can try out.\n",
    "\n",
    "# embed the query using the embed_query method.\n",
    "# this method works exactly the same as the embed_documents \n",
    "# except that it takes as input a single string\n",
    "query_embedding = np.array(embedding_func.embed_query(query))\n",
    "\n",
    "similarity_score = []\n",
    "for sentence_embedding in sentence_embeddings:\n",
    "    similarity_score.append(cosine_similarity(query_embedding, sentence_embedding))\n",
    "\n",
    "# Here we create a dataframe with the sentences and their similarity score\n",
    "# the main reason for this is that it renders nicer in jupyter notebooks.\n",
    "df = pd.DataFrame({\"sentences\": sentences, \"score\": similarity_score})\n",
    "# YOUR CODE HERE START: Use the df.sort_values method to sort the dataframe by the score column.\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "print(f\"Query: `{query}`\")\n",
    "print(\"Most similar sentences\")\n",
    "df"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: `What did she say?`\n",
      "Most similar sentences\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "                                         sentences     score\n",
       "0                 Merel said something about cats.  0.221253\n",
       "1                     Merel said dogs are awesome.  0.100491\n",
       "2  Samantha's buddies said to meet them in the bar  0.346419\n",
       "3                The cat is walking in the bedroom -0.016139\n",
       "4                   The kittens are in the bedroom -0.001760\n",
       "5           A dogs were running across the kitchen -0.016283\n",
       "6   The puppies were running around in the kitchen -0.050865"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Merel said something about cats.</td>\n",
       "      <td>0.221253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Merel said dogs are awesome.</td>\n",
       "      <td>0.100491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Samantha's buddies said to meet them in the bar</td>\n",
       "      <td>0.346419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The cat is walking in the bedroom</td>\n",
       "      <td>-0.016139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The kittens are in the bedroom</td>\n",
       "      <td>-0.001760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A dogs were running across the kitchen</td>\n",
       "      <td>-0.016283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The puppies were running around in the kitchen</td>\n",
       "      <td>-0.050865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c438a3a47837208e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction to vector databases\n",
    "Vector databases are a type of database that is optimized for similarity search.\n",
    "They don't need to search through all the items to find the most similar items.\n",
    "This makes them even faster than $O(n)$ search algorithms.\n",
    "\n",
    "In the remainder of this notebook, we will experiment with the [FAISS](https://github.com/facebookresearch/faiss) vector database.\n",
    "This is a very fast vector database from Facebook. \n",
    "It is also easy to use because all it takes to install it is `pip install faiss-cpu`.\n",
    "FAISS also nicely integrates with LangChain making it even easier to use.\n",
    "\n",
    "In the cell below, we do the following:\n",
    "1. We define our query and sentences.\n",
    "2. We create a vector database around the sentences using [FAISS.from_texts](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html#langchain.vectorstores.faiss.FAISS.from_texts) method.\n",
    "3. We search through the vector database using the [similarity_search](https://api.python.langchain.com/en/latest/vectorstores/langchain.vectorstores.faiss.FAISS.html#langchain.vectorstores.faiss.FAISS.similarity_search) method. We pass the query and the number of items we want to retrieve (`k`) as arguments. \n",
    "4. We print the results.\n",
    "\n",
    "\n",
    "Please run the cell below and do the following:\n",
    "1. Change the query and see how the results change. Does it act similar to the previous exercise?\n",
    "2. Change the number of items we want to retrieve (`k`). What happens if you set it to 1? What happens if you set it to `len(sentences)`?"
   ]
  },
  {
   "cell_type": "code",
   "id": "2b197b3210259e8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:37:44.838483Z",
     "start_time": "2025-03-06T11:37:44.772086Z"
    }
   },
   "source": [
    "query = \"what are the puppies doing?\"\n",
    "sentences = [\n",
    "    \"Merel said something about cats.\",\n",
    "    \"Merel said dogs are awesome.\",\n",
    "    \"Samantha's buddies said to meet them in the bar\",\n",
    "    \"The cat is walking in the bedroom\",\n",
    "    \"The kittens are in the bedroom\",\n",
    "    \"A dogs were running across the kitchen\",\n",
    "    \"The puppies were running around in the kitchen\",\n",
    "]\n",
    "\n",
    "vector_database = FAISS.from_texts(sentences, embedding=embedding_func)\n",
    "\n",
    "k = 10\n",
    "documents = vector_database.similarity_search(query, k=k)\n",
    "\n",
    "print(\"The result is a list of k sdocuments, sorted in order of relevance:\")\n",
    "print(documents)\n",
    "\n",
    "print()\n",
    "print(\"Each document has a page_content (the string that was embedded) and optionally metadata\")\n",
    "for doc in documents:\n",
    "    print(f\"- page_content=`{doc.page_content}`\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result is a list of k sdocuments, sorted in order of relevance:\n",
      "[Document(id='b383020d-577a-45de-8e09-e5d193e9bba6', metadata={}, page_content='The puppies were running around in the kitchen'), Document(id='788b3c5d-dcde-49ee-b39f-a7a8de232e96', metadata={}, page_content='Merel said dogs are awesome.'), Document(id='f1609149-3834-4640-937a-411acd1f17c8', metadata={}, page_content='A dogs were running across the kitchen'), Document(id='2562c31d-e6fd-4d85-bc85-dd29a3fc2663', metadata={}, page_content='The kittens are in the bedroom'), Document(id='bf4350e5-af0f-4f82-b410-71ba55547521', metadata={}, page_content=\"Samantha's buddies said to meet them in the bar\"), Document(id='33820044-7324-4a8e-a590-135355d643f3', metadata={}, page_content='Merel said something about cats.'), Document(id='cf41616d-01c9-4653-b86a-b81e24a2debb', metadata={}, page_content='The cat is walking in the bedroom')]\n",
      "\n",
      "Each document has a page_content (the string that was embedded) and optionally metadata\n",
      "- page_content=`The puppies were running around in the kitchen`\n",
      "- page_content=`Merel said dogs are awesome.`\n",
      "- page_content=`A dogs were running across the kitchen`\n",
      "- page_content=`The kittens are in the bedroom`\n",
      "- page_content=`Samantha's buddies said to meet them in the bar`\n",
      "- page_content=`Merel said something about cats.`\n",
      "- page_content=`The cat is walking in the bedroom`\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9f07727491a4e9ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2b: Introduction to metadata in vector databases\n",
    "The nice thing about vector databases is that they can also store metadata. This is extra information that is not embedded but can be used to filter the results. Additionally, you can also store additional information about the original document, such as:\n",
    "- The original document id.\n",
    "- The original document URL.\n",
    "- The original document title.\n",
    "- When was the document added to the database?\n",
    "- etc.\n",
    "\n",
    "Please run the cell below and do the following:\n",
    "- Change the metadata. Does it have any effect on the results?\n",
    "- Add some additional metadata and print it.\n",
    "- Add some additional metadata and filter the results based on this metadata. (for example, try printing only the documents with an even `original_document_id`)."
   ]
  },
  {
   "cell_type": "code",
   "id": "89e1f161d5aa5544",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T11:51:30.597487Z",
     "start_time": "2025-03-06T11:51:30.523351Z"
    }
   },
   "source": [
    "query = \"what are the puppies doing?\"\n",
    "sentences = [\n",
    "    \"Merel said something about cats.\",\n",
    "    \"Merel said dogs are awesome.\",\n",
    "    \"A dogs were running across the kitchen\",\n",
    "    \"The puppies were running around in the kitchen\",\n",
    "]\n",
    "metadatas = [\n",
    "    {\"original_document_id\": 1, },\n",
    "    {\"original_document_id\": 2},\n",
    "    {\"original_document_id\": 3, \"some_additional_key\": \"bla bla\"},\n",
    "    {\"original_document_id\": 4, \"are puppies fun?\": \"yes\"},\n",
    "]\n",
    "vector_database = FAISS.from_texts(sentences, metadatas=metadatas, embedding=embedding_func)\n",
    "\n",
    "k = 3\n",
    "documents = vector_database.similarity_search(query, k=k)\n",
    "\n",
    "print(\"Now Each document has a page_content and metadata\")\n",
    "for doc in documents:\n",
    "    print(f\"- page_content=`{doc.page_content}` metadata={doc.metadata}\")\n",
    "\n",
    "print(\"#\" * 80)\n",
    "# YOUR CODE HERE START: Try to filter the results based on the metadata. (e.g., print only the documents with even original_document_id)\n",
    "f = filter(lambda x: x.metadata is not None and \"some_additional_key\" in x.metadata, documents)\n",
    "for doc in f:\n",
    "    print(doc)\n",
    "# YOUR CODE HERE END"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now Each document has a page_content and metadata\n",
      "- page_content=`The puppies were running around in the kitchen` metadata={'original_document_id': 4, 'are puppies fun?': 'yes'}\n",
      "- page_content=`Merel said dogs are awesome.` metadata={'original_document_id': 2}\n",
      "- page_content=`A dogs were running across the kitchen` metadata={'original_document_id': 3, 'some_additional_key': 'bla bla'}\n",
      "################################################################################\n",
      "page_content='A dogs were running across the kitchen' metadata={'original_document_id': 3, 'some_additional_key': 'bla bla'}\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a463aed020473794",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2c: Metadata based title search\n",
    "In this exercise, we will search through a database of stories based on their title.\n",
    "\n",
    "We will do the following:\n",
    "1. For each story, we will split the story into sentences using the `.split(\".\")` method.\n",
    "2. We will then build a vector database from these sentences (with the name of the story they belong to stored as metadata).\n",
    "3. To find the most relevant story for a given query, the process is then as follows:\n",
    "    1. Embedd the query.\n",
    "    2. Search through the vector database to find the most similar sentences.\n",
    "    3. Count how often each story is mentioned in the results (based on the metadata of the found sentences).\n",
    "    4. Pick the story that is mentioned most often.\n",
    "    \n",
    "We have already given you a skeleton of the code. Your task is to fill in the missing parts marked with `# YOUR CODE HERE START` and `# YOUR CODE HERE END`."
   ]
  },
  {
   "cell_type": "code",
   "id": "60f5e99661d2783a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:20:31.120566Z",
     "start_time": "2025-03-06T12:20:30.677851Z"
    }
   },
   "source": [
    "stories = {\n",
    "    \"Snow the husky story\": \"Snow, the husky puppy, was born with eyes as blue as the winter sky. He loved bounding through the snow-covered forest, his paws leaving tiny imprints behind. With each playful leap, Snow brought joy and warmth to all who crossed his path, reminding them that even in the coldest of times, there is always a glimmer of happiness.\",\n",
    "    \"Siamese twins story\": \"Luna and Stella, the Siamese twins cats, were inseparable from the moment they were born. With their striking blue eyes and sleek coats, these cats were a sight to behold. Their synchronized movements and playful antics enchanted everyone they met, leaving a lasting impression that two Siamese cats are always better than one.\",\n",
    "    \"Biking story\": \"As the sun kissed the horizon, Sarah hopped on her bike, ready for an adventure. With the wind in her hair and the pedals beneath her feet, she embarked on a journey of freedom and exploration. Each mile brought her closer to new sights, fresh air, and the exhilarating feeling of the open road.\",\n",
    "}\n",
    "\n",
    "sentences = [] # A list of str, where each str is a sentence.\n",
    "metadatas = [] # A list of dict, which contain the story name of the corresponding sentence. For example, {\"story_name\": \"Snow the husky\"}.\n",
    "\n",
    "\n",
    "for story_name, story in stories.items():\n",
    "    # YOUR CODE HERE START: split the story into sentences and store each sentence in the sentences list.\n",
    "    story_sentences = filter(lambda x: len(x) > 5, story.split(\".\"))\n",
    "    print(story_sentences)\n",
    "    for sentence in story_sentences:\n",
    "        sentences.append(sentence.strip())\n",
    "        metadatas.append({ \"story_name\": story_name })\n",
    "\n",
    "    # YOUR CODE HERE END\n",
    "\n",
    "assert len(sentences) > 0, f\"It looks like you forgot to add the sentences.\"\n",
    "assert len(sentences) == len(metadatas), f\"Meta data and sentences must have same length but {len(metadatas)} != {len(sentences)}\"\n",
    "\n",
    "# YOUR CODE HERE START: Create a vector database around the sentences and their metadata. (Hint: use the FAISS.from_texts method)\n",
    "vector_database = FAISS.from_texts(texts=sentences, metadatas=metadatas, embedding=embedding_func)\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "# A query and the corresponding answer it should produce.\n",
    "queries_and_answer = {\n",
    "    \"Give me a story about biking\":  \"Biking story\",\n",
    "    \"Give me a story about Siamese cats\": \"Siamese twins story\",\n",
    "    \"Give me a story about dogs\": \"Snow the husky story\",\n",
    "}\n",
    "\n",
    "# Loop through each query:\n",
    "for query, answer in queries_and_answer.items():\n",
    "\n",
    "    # For each query, Keep a counter of the recommendations.\n",
    "    recommendation_per_story = {story_name: 0 for story_name in stories.keys()}\n",
    "\n",
    "    # Return sentences similar to the query\n",
    "    k = 3\n",
    "    similar_sentences = vector_database.similarity_search(query, k=k)\n",
    "    # YOUR CODE HERE START: for each document, count how often each story is mentioned.\n",
    "    for sentence in similar_sentences:\n",
    "        recommendation_per_story[sentence.metadata['story_name']] += 1\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    story_mentioned_most_often = max(recommendation_per_story, key=recommendation_per_story.get)\n",
    "    print(f\"Testing query: `{query}`. Got the following mentions: {recommendation_per_story}\" )\n",
    "    assert story_mentioned_most_often == answer, f\"Expected `{answer}` but got `{story_mentioned_most_often}` for query `{query}.\"\n",
    "    print(\"✅ Passed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<filter object at 0x17c8cc250>\n",
      "<filter object at 0x17c8a9420>\n",
      "<filter object at 0x17c72b1f0>\n",
      "Testing query: `Give me a story about biking`. Got the following mentions: {'Snow the husky story': 0, 'Siamese twins story': 0, 'Biking story': 3}\n",
      "✅ Passed!\n",
      "Testing query: `Give me a story about Siamese cats`. Got the following mentions: {'Snow the husky story': 0, 'Siamese twins story': 3, 'Biking story': 0}\n",
      "✅ Passed!\n",
      "Testing query: `Give me a story about dogs`. Got the following mentions: {'Snow the husky story': 2, 'Siamese twins story': 1, 'Biking story': 0}\n",
      "✅ Passed!\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbabb888db5d2525",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## PyData talks\n",
    "Now we know how to build a FAISS vector database with metadata and how to search through it.\n",
    "Let's try to build a vector database around the PyData talks and try to find the most relevant talks for a given query.\n",
    "\n",
    "Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a592d197a0ca5c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:25:07.498863Z",
     "start_time": "2025-03-06T12:25:07.494586Z"
    }
   },
   "source": [
    "with open(\"pydata.json\", \"r\") as f:\n",
    "    talks = json.load(f)[\"talks\"]\n",
    "    titles = [talk[\"title\"] for talk in talks]\n",
    "    abstracts = [talk[\"abstract\"] for talk in talks]\n",
    "    descriptions = [talk[\"description\"] for talk in talks]"
   ],
   "outputs": [],
   "execution_count": 51
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9623382ea21ca654",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2d: Search through PyData talks\n",
    "In this exercise, we will do the following:\n",
    "1. Create a vector database around the titles, abstracts, and descriptions of the talks. For each item, we also store the following metadata:\n",
    "    - The `title` of the talk.\n",
    "    - The `talk_idx`, which is the index of the talk in the `talks` list.\n",
    "    - The `type` of text (title, abstract, or description) so we know where the text came from.\n",
    "2. We then search the vector database based on the query and print the results.\n",
    "\n",
    "We have already given you a skeleton of the code. Your task is to fill in the missing parts marked with `# YOUR CODE HERE START` and `# YOUR CODE HERE END`.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a492d266df88b2e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:39:17.982509Z",
     "start_time": "2025-03-06T12:39:17.495632Z"
    }
   },
   "source": [
    "texts = [] # A list of str \n",
    "metadatas = [] # A list of dict\n",
    "\n",
    "for talk_idx in range(len(talks)):\n",
    "    title = titles[talk_idx]\n",
    "    abstract = abstracts[talk_idx]\n",
    "    description = descriptions[talk_idx]\n",
    "    \n",
    "    texts.append(title)\n",
    "    metadatas.append({\"title\": title, \"talk_idx\": talk_idx, \"form\": \"title\"})\n",
    "\n",
    "    # YOUR CODE HERE START: Add the abstract and its metadata to the texts and metadatas lists.\n",
    "    texts.append(abstract)\n",
    "    metadatas.append({\"abstract\": abstract, \"talk_idx\": talk_idx, \"form\": \"abstract\"})\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    # YOUR CODE HERE START: Add the description and its metadata to the texts and metadatas lists.\n",
    "    texts.append(description)\n",
    "    metadatas.append({\"description\": description, \"talk_idx\": talk_idx, \"form\": \"description\"})\n",
    "    # YOUR CODE HERE END:\n",
    "\n",
    "assert len(texts) == len(metadatas)\n",
    "\n",
    "# YOUR CODE HERE START: Create a vector database around the texts and their metadata (Hint: use the FAISS.from_texts method).\n",
    "vector_database = FAISS.from_texts(texts, metadatas=metadatas, embedding=embedding_func)\n",
    "# YOUR CODE HERE END:"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "cell_type": "code",
   "id": "7ab3b8b5b8e44fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T12:48:21.229064Z",
     "start_time": "2025-03-06T12:48:20.898026Z"
    }
   },
   "source": [
    "query = 'which talks are about LLM?'\n",
    "# query = 'which talks are about data engineering?' # This is an alternative query that you can try out.\n",
    "documents = vector_database.similarity_search(query)\n",
    "print(len(documents))\n",
    "\n",
    "for document in documents:\n",
    "    # YOUR CODE HERE START: print the title and the text where the query was found.\n",
    "    print(document.page_content)\n",
    "    # print(document.metadata)\n",
    "    # print(document.metadata.get(\"title\"))\n",
    "    # print(document.metadata.get(\"abstract\"))\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    print(\"#\" * 80 + \"\\n\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "Mind the language: how to monitor NLP and LLM in production\n",
      "################################################################################\n",
      "\n",
      "Pick your next hot LLM prompt using a Bayesian tournament! Get a quick LLM dopamine hit with a side of decision theory vegetables. It's Bayesian Thunderdome: many prompts enter, one prompt leaves.\n",
      "################################################################################\n",
      "\n",
      "How do you chose the best LLM prompt systematically beyond guessing and vibes? Use the winner of a Bayesian tournament! Get a quick dopamine hit from fun LLM prompt magic with a side of Bayesian decision theory vegetables. If you are doing stuff with LLMs — you'll get a serious tool to improve your prompting game. If you're not using LLMs — you'll learn about Bayesian tournaments. They are not well known but have wide applicability: they help you optimally choose a winner using a minimal number of matches.\n",
      "################################################################################\n",
      "\n",
      "The recent developments in LLMs are an interesting area for us to explore to improve our recommendations. However, running LLMs in production is unfortunately not always feasible. The associated costs may be too high, and running code from third parties in your daily pipeline may be undesirable. And then there’s data privacy -  or, in our case, intellectual copyright - to be considered as well. \r\n",
      "\r\n",
      "So how can you reap the benefits of an LLM, without exposing yourself or your company to some of these major downsides? \r\n",
      "\r\n",
      "We utilized LLMs to generate custom, tailor-made datasets for our literary feature detection models to train on. This allowed us to benefit from the high performance of large language models, without continued reliance on external parties such as OpenAI or Google.\r\n",
      "\r\n",
      "While you may think LLMs are not as effective for languages other than English, we’ve seen major improvements in several of our models. \r\n",
      "\r\n",
      "In this talk, we’ll highlight:\r\n",
      "- A note on recommenders: Why does Goodreads recommender not work for me, while Spotify’s Discover Weekly is so good?\r\n",
      "- Different methods of getting data from books\r\n",
      "- Iterative process of creating a dataset using an LLM and retraining our models\r\n",
      "- Some notes on intellectual property and evaluation of models.\n",
      "################################################################################\n",
      "\n"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "cell_type": "markdown",
   "id": "979d741b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
