{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c05fafa1fe34c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 4 - Introduction to OpenAI API and prompting\n",
    "OpenAI makes their models available via an [REST API](https://platform.openai.com/docs/api-reference) and multiple SDKs.\n",
    "In this exercise, we will explore how to use the Python SDK to interact with these models.\n",
    "\n",
    "Before we can start, we must create an instance of the OpenAI client.\n",
    "We can do this by providing the `api_key` and `azure_endpoint` to the `AzureOpenAI` class.\n",
    "\n",
    "If you are using Github codespaces, you will already have access to the OpenAI secrets and can proceed to run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f6560238786a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.openai_utils import get_openai_client\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# This reads the .env file in your project and transforms its content into env variables.\n",
    "# This way you don't have to hard code your secrets.\n",
    "dotenv.load_dotenv()\n",
    "# Here we create the client.\n",
    "client = get_openai_client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66a30000",
   "metadata": {},
   "source": [
    "\n",
    "However, if you are not using Github codespaces, before you can run te code above, you must have an `.env` file at the root of your project.\n",
    "There should be an example of this file named `.env.example`.\n",
    "Copy this file and rename it to `.env`.\n",
    "Then, ask your instructor for any missing secrets.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fce235371fcc0cee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction OpenAI API\n",
    "OpenAI offers two types of text generation: text-completion and chat completion.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f712b574070d3a1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Completion API\n",
    "This API is most similar to the GPT-2 API we saw in the previous text generation exercise.\n",
    "This API works as follows:\n",
    "- You specify the text completion model (`gpt-35-turbo-instruct`, `babbage-002`, etc.) via the `model` parameter.\n",
    "- You specify the text to be completed via the `prompt` parameter.\n",
    "- Optionally, you specify some stop words. Once the generated text ends in one of these words, it will automatically stop generating.\n",
    "\n",
    "The response contains a lot more information about the generation process. However, we are mainly interested in the generated text, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead280516a12393",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"AI:\", \"Human:\", \"####\"]\n",
    "n=50\n",
    "text = \"\"\"\n",
    "You are an AI assistant that helps people with their daily tasks.\n",
    "Lines starting with 'AI:' are the things the AI assistant says.\n",
    "Lines starting with 'Human:' are the things the human says.\n",
    "Lines starting with '####' mean that the conversation is over and a new one is starting. Things discussed in previous conversations are not remembered.\n",
    "\n",
    "Human: Give me some ideas for dinner tonight.\n",
    "AI:\"\"\".strip() # feel free to change this text to see what happens.\n",
    "\n",
    "completion = client.completions.create(\n",
    "    model=os.environ[\"GPT_35_TURBO_INSTRUCT_MODEL_NAME\"],\n",
    "    prompt=text,\n",
    "    stop=stop_words,\n",
    "    max_tokens=n\n",
    ")\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "185ce4346dd77325",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Chat API\n",
    "Besides text generation, OpenAI also offers APIs around its chat models, such as Chat-GPT (aka gpt-35-turbo).\n",
    "As you might expect, these models want their inputs in chat format.\n",
    "This changes their function call slightly.\n",
    "Instead of text, we input messages.\n",
    "A message consists of `content,` which is the text of the message, and a `role.`\n",
    "The role has to be one of the following values:\n",
    "- `system`: This contains the system instructions the model should follow during the conversation.\n",
    "- `user`:  This means that the message is something the user said.\n",
    "- `assistant`: This means that the message is something the assistant/model said.\n",
    "\n",
    "Feel free to try it out in the bellow cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f541b92691463",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":  \"system\",\n",
    "            \"content\": \"You are a ChefBot that provides recipes.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a recipe for a pasta dish.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Sure, do you have any allergies\",\n",
    "        },\n",
    "         {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"No I don't\",\n",
    "        },\n",
    "    ],\n",
    "    model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92ed5252814c515e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Exercise 4a - base model vs instruction fine-tuned model\n",
    "The OpenAI API also gives us access to the base models of GPT-3. The base models are not yet fined tuned to follow instructions or a chat format. So let's explore how much of an impact this instruction fine-tunning has on the stearability of large langue models. \n",
    "\n",
    "The goal is to get both the base model and instruction fine-tuned to tell a joke. Tell only thing you are allowed to change is the prompt text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55b4c89a78bae327",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Base model\n",
    "Here, we use the GPT-3 base model [Babbage](https://platform.openai.com/docs/models/gpt-base), which is of a similar size to gpt-35-turbo to make the comparison fair.\n",
    "Change the text in the cell and run the cell to see the generated text.\n",
    "\n",
    "Remember, the goal is to design a prompt that generates a new and funny joke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f9c19eac860273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "completion = client.completions.create(\n",
    "    model=os.environ[\"BABBAGE_MODEL_NAME\"],\n",
    "    prompt=text, # Define this variable,\n",
    "    # max_token = 1000\n",
    ")\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b7d4a5ca9d2ae8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Instruction fine-tuned model\n",
    "Here, we use the instructed fine-tuned model named [gpt-35-turbo-instruct](https://platform.openai.com/docs/models/gpt-3-5), which is a text completion version of ChatGPT.\n",
    "\n",
    "Remember, the goal is to design a prompt that generates a new and funny joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f71905de170cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "completion = client.completions.create(\n",
    "    model=os.environ[\"GPT_35_TURBO_INSTRUCT_MODEL_NAME\"],\n",
    "    prompt=text,  # Define this variable\n",
    ")\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6d6fc23a0acdcf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Chat fine-tuned model\n",
    "Here, we use the chat fine-tuned model named [gpt-35-turbo](https://platform.openai.com/docs/models/gpt-3-5), which is the same model ChatGPT uses in the backend.\n",
    "With this model, you have to format your prompt as a list of message objects `{'role': system|user|assistant, 'content': ...}`. \n",
    "\n",
    "Remember, the goal is to design a prompt that generates a new and funny joke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428970fc4880509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages, # Define this variable\n",
    "    model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1234e2ea4aa895d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optional: Exercise 4b - base model vs instruction fine-tuned model\n",
    "Let's make a CLI-based version of chat GPT.\n",
    "Below, you will find some boilerplate code that takes care of asking the user for input and printing the conversation.\n",
    "However, there are still a few things missing like:\n",
    "- The content of the inital system prompt.\n",
    "- The API call to `client.chat.completions.create` which leads to a response from the chatbot based on the conversation so far.\n",
    "\n",
    "\n",
    "Can you finish it?\n",
    "\n",
    "<mark>Note: The text box should appear at the top of your IDE (VS Code) window.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edce517882df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# The initial system prompt.\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":  \"system\",\n",
    "        # YOUR CODE HERE START\n",
    "        # YOUR CODE HERE END\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    # Get the user's input.\n",
    "    user_message = input('User:').strip()\n",
    "    \n",
    "    # Check whether to continue.\n",
    "    if len(user_message) == 0 or user_message == \"exit\":\n",
    "        print(\"exiting...\")\n",
    "        break\n",
    "    \n",
    "    # Remember the users response.\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message,\n",
    "    })\n",
    "\n",
    "    # Get the response from the GPT model.\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "    \n",
    "    # Remember the assistant's response.\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_message,\n",
    "    })\n",
    "\n",
    "    # Print the user input and response.\n",
    "    print(\"User:\", user_message)\n",
    "    print(\"AI:\", assistant_message)\n",
    "\n",
    "    # We need to wait a little bit of time for the text to render.\n",
    "    time.sleep(0.5) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}