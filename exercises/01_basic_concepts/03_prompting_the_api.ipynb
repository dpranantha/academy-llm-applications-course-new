{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66c05fafa1fe34c7",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3 - Introduction to OpenAI API and prompting\n",
    "OpenAI makes their models available via an [REST API](https://platform.openai.com/docs/api-reference) and multiple SDKs.\n",
    "In this exercise, we will explore how to use the Python SDK to interact with these models.\n",
    "\n",
    "Before we can start, we must create an instance of the OpenAI client.\n",
    "We can do this by providing the `api_key` and `azure_endpoint` to the `AzureOpenAI` class.\n",
    "\n",
    "If you are using Github codespaces, you will already have access to the OpenAI secrets and can proceed to run the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f6560238786a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.openai_utils import get_openai_client\n",
    "import dotenv\n",
    "import os\n",
    "\n",
    "# This reads the .env file in your project and transforms its content into env variables.\n",
    "# This way you don't have to hard code your secrets.\n",
    "dotenv.load_dotenv()\n",
    "# Here we create the client.\n",
    "client = get_openai_client()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66a30000",
   "metadata": {},
   "source": [
    "\n",
    "However, if you are not using Github codespaces, before you can run te code above, you must have an `.env` file at the root of your project.\n",
    "There should be an example of this file named `.env.example`.\n",
    "Copy this file and rename it to `.env`.\n",
    "Then, ask your instructor for any missing secrets.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fce235371fcc0cee",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Introduction OpenAI API\n",
    "OpenAI offers two types of text generation: text-completion and chat completion.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f712b574070d3a1a",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Completion/Instruction-following API\n",
    "This API is most similar to the GPT-2 API we saw in the previous text generation exercise.\n",
    "This API works as follows:\n",
    "- You specify the text completion model (e.g. the now expired `babbage` model) or instruction-following model (e.g. `gpt-35-turbo-instruct`)via the `model` parameter.\n",
    "- You specify the text to be completed via the `prompt` parameter.\n",
    "- Optionally, you specify some stop words. Once the generated text ends in one of these words, it will automatically stop generating.\n",
    "\n",
    "The response contains a lot more information about the generation process. However, we are mainly interested in the generated text, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aead280516a12393",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Lines starting with 'AI:' are the things the AI assistant says.\n",
    "Lines starting with 'Human:' are the things the human says.\n",
    "Lines starting with '####' mean that the conversation is over and a new one is starting. Things discussed in previous conversations are not remembered.\n",
    "\n",
    "Human: Give me some ideas for dinner tonight.\n",
    "AI:\"\"\".strip() # feel free to change this text to see what happens.\n",
    "\n",
    "stop_words = [\"AI:\", \"Human:\", \"####\"]\n",
    "n=100\n",
    "\n",
    "completion = client.completions.create(\n",
    "    model=os.environ[\"GPT_35_TURBO_INSTRUCT_MODEL_NAME\"],\n",
    "    prompt=text,\n",
    "    stop=stop_words,\n",
    "    max_tokens=n\n",
    ")\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bfeda4",
   "metadata": {},
   "source": [
    "<mark>**Question:**</mark> **Look what happens when we instruct the model how to act. Why is this the case?**\n",
    "\n",
    "<br>  \n",
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "  \n",
    "This instruction has triggered one of GPT's input guardrails.\n",
    "\n",
    "Input guardrails aim to prevent inappropriate content getting to the LLM in the first place - some common use cases are:\n",
    "\n",
    "- Topical guardrails:\u00a0Identify when a user asks an off-topic question and give them advice on what topics the LLM can help them with.\n",
    "\n",
    "- Jailbreaking:\u00a0Detect when a user is trying to hijack the LLM and override its prompting.\n",
    "\n",
    "- Prompt injection:\u00a0Pick up instances of prompt injection where users try to hide malicious code that will be executed in any downstream functions the LLM executes.\n",
    "\n",
    "All of these measures are preventative, running either before or in parallel with the LLM, and triggering your application to behave differently if one of these criteria are met.\n",
    "\n",
    "    \n",
    "</details>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f7b78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\"\n",
    "# You are a helpful AI assistant that helps people with their daily tasks.\n",
    "# Lines starting with 'AI:' are the things the AI assistant says.\n",
    "# Lines starting with 'Human:' are the things the human says.\n",
    "# Lines starting with '####' mean that the conversation is over and a new one is starting. Things discussed in previous conversations are not remembered.\n",
    "\n",
    "# Human: Give me some ideas for dinner tonight.\n",
    "# AI:\"\"\".strip() # feel free to change this text to see what happens.\n",
    "\n",
    "# completion = client.completions.create(\n",
    "#     model=os.environ[\"GPT_35_TURBO_INSTRUCT_MODEL_NAME\"],\n",
    "#     prompt=text,\n",
    "#     stop=stop_words,\n",
    "#     max_tokens=n\n",
    "# )\n",
    "# print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "185ce4346dd77325",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Chat API\n",
    "Besides text generation, OpenAI also offers APIs around its chat models, such as Chat-GPT.\n",
    "As you might expect, these models want their inputs in chat format.\n",
    "This changes their function call slightly.\n",
    "Instead of text, we input messages.\n",
    "A message consists of `content,` which is the text of the message, and a `role.`\n",
    "The role has to be one of the following values:\n",
    "- `system`: This contains the system instructions the model should follow during the conversation.\n",
    "- `user`:  This means that the message is something the user said.\n",
    "- `assistant`: This means that the message is something the assistant/model said.\n",
    "\n",
    "Feel free to try it out in the bellow cell. We are using a GPT-3.5 Turbo model: a fast, inexpensive model for chat tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f541b92691463",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":  \"system\",\n",
    "            \"content\": \"You are a ChefBot that provides recipes.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a recipe for a pasta dish.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Sure, do you have any allergies\",\n",
    "        },\n",
    "         {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"No I don't\",\n",
    "        },\n",
    "    ],\n",
    "    model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4dedf",
   "metadata": {},
   "source": [
    "OpenAI have now released GPT-4o mini: a compact, more cost-effective version of OpenAI\u2019s GPT-4o model, designed to replace GPT-3.5 Turbo. It excels in textual comprehension, coding, and math, outperforming GPT-3.5 on benchmarks like MMLU and HumanEval. With a larger 128,000-token context window, it\u2019s highly effective for handling extensive text documents and also has enhanced safety features.\n",
    "\n",
    "Compare the response you get when using the GPT-4o mini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519808f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\":  \"system\",\n",
    "            \"content\": \"You are a ChefBot that provides recipes.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Give me a recipe for a pasta dish.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Sure, do you have any allergies\",\n",
    "        },\n",
    "         {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"No I don't\",\n",
    "        },\n",
    "    ],\n",
    "    model=os.environ[\"GPT_4_MODEL_NAME\"],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92ed5252814c515e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "# Exercise 3a - base model vs instruction fine-tuned model\n",
    "The OpenAI API gives us access to the base models of GPT-3. \n",
    "\n",
    "The base models are not yet fined tuned to follow instructions or a chat format. So let's explore how much of an impact this instruction fine-tunning has on the stearability of large langue models. \n",
    "\n",
    "The goal is to get both the base model and instruction fine-tuned to tell a joke. Tell only thing you are allowed to change is the prompt text."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b7d4a5ca9d2ae8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Instruction fine-tuned model\n",
    "Here, we use the instructed fine-tuned model named [gpt-35-turbo-instruct](https://platform.openai.com/docs/models/gpt-3-5), which is a text completion version of ChatGPT.\n",
    "\n",
    "Remember, the goal is to design a prompt that generates a new and funny joke."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f71905de170cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "completion = client.completions.create(\n",
    "    model=os.environ[\"GPT_35_TURBO_INSTRUCT_MODEL_NAME\"],\n",
    "    prompt=text,  # Define this variable\n",
    ")\n",
    "print(completion.choices[0].text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d6d6fc23a0acdcf0",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Chat fine-tuned model 3.5\n",
    "Here, we use a chat fine-tuned model named [gpt-35-turbo](https://platform.openai.com/docs/models/gpt-3-5), which was the model ChatGPT originally used.\n",
    "With this model, you have to format your prompt as a list of message objects `{'role': system|user|assistant, 'content': ...}`. \n",
    "\n",
    "Remember, the goal is to design a prompt that generates a new and funny joke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a428970fc4880509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages, # Define this variable\n",
    "    model=os.environ[\"GPT_35_CHAT_MODEL_NAME\"],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2be4e",
   "metadata": {},
   "source": [
    "### Chat fine-tuned model 4o-mini\n",
    "Here, we use a chat fine-tuned model named [gpt-4o-mini](https://platform.openai.com/docs/models/gpt-4o-mini), which is the model ChatGPT currently uses.\n",
    "With this model, you have to format your prompt as a list of message objects `{'role': system|user|assistant, 'content': ...}`. \n",
    "\n",
    "Remember, the goal is to design a prompt that generates a new and funny joke.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35863811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE START\n",
    "# YOUR CODE HERE END\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    messages=messages, # Define this variable\n",
    "    model=os.environ[\"GPT_4_MODEL_NAME\"],\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1234e2ea4aa895d6",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optional: Exercise 3b - base model vs instruction fine-tuned model\n",
    "Let's make a CLI-based version of chat GPT.\n",
    "Below, you will find some boilerplate code that takes care of asking the user for input and printing the conversation.\n",
    "However, there are still a few things missing like:\n",
    "- The content of the inital system prompt.\n",
    "- The API call to `client.chat.completions.create` which leads to a response from the chatbot based on the conversation so far.\n",
    "\n",
    "\n",
    "Can you finish it?\n",
    "\n",
    "<mark>Note: The text box should appear at the top of your IDE (VS Code) window.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37edce517882df0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# The initial system prompt.\n",
    "messages = [\n",
    "    {\n",
    "        \"role\":  \"system\",\n",
    "        # YOUR CODE HERE START\n",
    "        # YOUR CODE HERE END\n",
    "    }\n",
    "]\n",
    "\n",
    "while True:\n",
    "    # Get the user's input.\n",
    "    user_message = input('User:').strip()\n",
    "    \n",
    "    # Check whether to continue.\n",
    "    if len(user_message) == 0 or user_message == \"exit\":\n",
    "        print(\"exiting...\")\n",
    "        break\n",
    "    print(1)\n",
    "    # Remember the users response.\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_message,\n",
    "    })\n",
    "\n",
    "    # Get the response from the GPT model.\n",
    "    # YOUR CODE HERE START\n",
    "    # YOUR CODE HERE END\n",
    "    print(2)\n",
    "    # Remember the assistant's response.\n",
    "    messages.append({\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": assistant_message,\n",
    "    })\n",
    "\n",
    "    # Print the user input and response.\n",
    "    print(\"User:\", user_message)\n",
    "    print(\"AI:\", assistant_message)\n",
    "\n",
    "    # We need to wait a little bit of time for the text to render.\n",
    "    time.sleep(0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f5d537",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}