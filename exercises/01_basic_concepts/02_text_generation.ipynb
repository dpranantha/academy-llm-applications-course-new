{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a61ebce70e82a34b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 2 - Introduction to LLM-based text generation\n",
    "In this notebook, we will explore how a large language model generates the text via next-word prediction. We will use the [GPT-2](https://huggingface.co/gpt2) model since it is relatively small and can be run on small hardware like your laptop. However, the same principles apply to larger models like GPT-3/GPT-4.\n",
    "\n",
    "GPT-2 is hosted, free, and available on Hugging Face, so we will use this library to access it. To do this, we need to import the following utility function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f72239939e2a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "616340199b2d331e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the following code, we load the GPT-2 tokenizer and model. If you run this for the first time, it will download the model and tokenizer. This might take a little bit of time.\n",
    "However, it only has to do this once. After that, it will be cached on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a5ca82377b7b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beeedeb8d88daf80",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2a - Next word prediction\n",
    "Let's explore the next word prediction of the capabilities of GPT-2.\n",
    "Run the cells below to see the next word prediction distribution for the text in the `text` variable.\n",
    "Play around with the different parameters and see what happens.\n",
    "Then ask yourself the following questions:\n",
    "- What happens if you make small changes to the text? For example, can you replace 'go' with 'visit'?\n",
    "- What happens if you make the temperature very low? E.g., 0.0001 or even 0?\n",
    "- What happens if you make the temperature very high? E.g., 2 or even 5?\n",
    "- What happens if you change the top_k parameter?\n",
    "- What happens if you change the top_p parameter to a very low value? E.g., 0.1 or even 0.01? (Note: make sure you set the temperature to 1)\n",
    "- What happens if you change the top_p parameter to a high value? E.g., 0.9 or even 0.99? (Note: make sure you set the temperature to 1)\n",
    "\n",
    "To help you with this, we have created the following utility functions.\n",
    "- `get_probs_next_word_top_k`: Given a text, this function returns the `k` most likely next words and their probabilities.\n",
    "- `get_probs_next_word_top_p`: Given a text, this function returns the next words and their probabilities until the cumulative probability of the next word is larger than the `top_p`.\n",
    "- `plot_probabilities`: Plots the probabilities of the next words as a bar chart.\n",
    "\n",
    "Let's import these functions and try them out.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4465da7dfc1f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_in_production.huggingface_utils import get_probs_next_word_top_k, get_probs_next_word_top_p\n",
    "from llm_in_production.visualization_utils import plot_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65330142754f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, do you want to go the\"\n",
    "k = 20 # play with this parameter\n",
    "temperature = 1 # play with this parameter\n",
    "renormalize = True # If true, it shows the probabilities as to how they will be sampled. If false, it shows the original probabilities.\n",
    "\n",
    "topk_words, topk_probs = get_probs_next_word_top_k(tokenizer, model, text, k=k, temperature=temperature)\n",
    "plot_probabilities(topk_words, topk_probs, renormalize=renormalize, title=f\"Top-k words with their probabilities (k={k})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3455001c41a3c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, do you want to go to the\"\n",
    "top_p = 0.2 # play with this parameter\n",
    "temperature = 1 # play with this parameter\n",
    "renormalize = False # If true, it shows the probabilities as to how they will be sampled. If false, it shows the original probabilities.\n",
    "\n",
    "topk_words, topk_probs = get_probs_next_word_top_p(tokenizer, model, text, top_p=top_p, temperature=temperature)\n",
    "plot_probabilities(topk_words, topk_probs, renormalize=renormalize, title=f\"Top-p words with their probabilities (p={top_p})\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "401116305b6834e9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 2b - Text generation\n",
    "We have now seen what the effect of the different parameters is on the next-word prediction distribution.\n",
    "Now, let's go a step further and generate more than one word. To do this, we use the `generate` function of the model.\n",
    "In this function, we can again change the `temperature,` `top_p`, and `top_k` parameters. \n",
    "\n",
    "Run the cell below to generate a sequence that completes the `text` variable.\n",
    "Play around with the different parameters and see what happens.\n",
    "Then ask yourself the following questions:\n",
    "- What happens if you make small changes to the text? For example, can you replace 'go' with 'visit'?\n",
    "- What happens if you make the temperature very low? E.g., 0.0001 or even 0?\n",
    "- What happens if you make the temperature very high? E.g., 2 or even 5?\n",
    "- What happens if you change the top_k parameter?\n",
    "- What happens if you change the top_p parameter to a very low value? E.g., 0.1 or even 0.01? (Note: make sure you set the temperature to 1)\n",
    "- What happens if you change the top_p parameter to a high value? E.g., 0.9 or even 0.99? (Note: make sure you set the temperature to 1)\n",
    "- What happens if you generate longer sequences? (What happens with the quality of the text?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df6df40d76fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hi, do you want to go the\" # feel free to change this text\n",
    "top_k = 20 # play with this parameter\n",
    "top_p = 1 # play with this parameter\n",
    "temperature = 0.1 # play with this parameter\n",
    "max_new_tokens = 25 # Increase this parameter to generate more text, but remember that the larger the text, the longer it takes to generate.\n",
    "eps = 1e-9 # small number to avoid log(0)\n",
    "\n",
    "# Here we encode the text to tokens the model understands\n",
    "tokens = tokenizer.encode_plus(text, return_tensors='pt')\n",
    "# Here we generate the text\n",
    "result = model.generate(\n",
    "    **tokens, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    do_sample=True, # If this is False, it uses greedy selects (token with max prob).\n",
    "    top_k=top_k, \n",
    "    top_p=top_p,\n",
    "    temperature=temperature + eps, \n",
    "    pad_token_id=model.config.eos_token_id, \n",
    ")\n",
    "# Here we decode the tokens back to strings\n",
    "generate_text = tokenizer.decode(result[0])\n",
    "print(generate_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2fcd70",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}